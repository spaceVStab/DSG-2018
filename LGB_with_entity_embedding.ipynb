{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fc654e2184fa4bb9958ac5975337500214dafeb","collapsed":true},"cell_type":"code","source":"pd.__version__","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#audio_utils\nimport numpy as np\n\ndef select_feature_func(feature_name):\n    if feature_name == 'aqibsaeed_1':\n        return get_feature_aqibsaeed_1\n    elif feature_name == 'mfcc':\n        return get_feature_mfcc\n\ndef get_feature_mfcc(X, sr, n_mfcc=13):\n    import librosa\n    mfcc = librosa.feature.mfcc(y=X, sr=sr, n_mfcc=n_mfcc)\n    return mfcc\n\ndef get_feature_aqibsaeed_1(X, sr, au_path=None):\n    \"\"\"\n    http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/\n    \"\"\"\n    import librosa\n    if au_path is not None:\n        X, sr = librosa.load(au_path)\n    stft = np.abs(librosa.stft(X))\n    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)\n    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n    mel = np.mean(librosa.feature.melspectrogram(X, sr=sr).T,axis=0)\n    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sr).T,axis=0)\n    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sr).T,axis=0)\n    feature = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n    return feature\n\ndef get_feature_aqibsaeed_conv(X, sr, au_path=None):\n    \"\"\"\n    http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/\n    \"\"\"\n    import librosa\n    def windows(data, window_size):\n        start = 0\n        while start < len(data):\n            yield start, start + window_size\n            start += (window_size / 2)\n    bands = 60\n    frames = 41\n    window_size = 512 * (frames - 1)\n    for (start,end) in windows(X, window_size):\n        if(len(X[start:end]) == window_size):\n            signal = X[start:end]\n            melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n            logspec = librosa.logamplitude(melspec)\n            logspec = logspec.T.flatten()[:, np.newaxis].T\n            log_specgrams.append(logspec)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d87fba8c20656a971d3fd4172c9edcdf0cd8bb76"},"cell_type":"code","source":"#cache utils\ndef name2path(name):\n    \"\"\"\n    Replace '/' in name by '_'\n    \"\"\"\n    return name.replace(\"/\", \"-\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"46f78ce3ff365e72f22f0d1aae4f859711f5c731"},"cell_type":"code","source":"#config utils\n# -*- coding:utf-8 -*-\n\ndef load_json(path):\n    import json\n    \"\"\"\n    支持以//开头的注释\n    \"\"\"\n    lines = []\n    with open(path) as f:\n        for row in f.readlines():\n            if row.strip().startswith(\"//\"):\n                continue\n            lines.append(row)\n    return json.loads(\"\\n\".join(lines))\n\ndef get_config_value(config, key, default_value, value_types, required=False, config_name=None):\n    \"\"\"\n\n    Parameters\n    ----------\n    config: dict\n        Config dictionary\n    key: str\n        Config's key\n    default_value: str\n        Default value when key is absent in config\n    value_types: Type or List of Types\n       if not None, should check value belongs one value_types\n    required: bool\n        if the key is required in config\n    config_name: str\n        used for debug\n    \"\"\"\n    if config_name is not None:\n        log_prefix = \"[{}] \".format(config_name)\n    else:\n        log_prefix = \"\"\n    if required and not key in config:\n        raise ValueError(\"{}config={}, key={} is absent but it's required !!!\".format(log_prefix, config, key))\n    elif not key in config:\n        return default_value\n    value = config[key]\n    # value type check\n    if value is not None:\n        value_type_match = True\n        if value_types is None:\n            value_types = []\n        elif not isinstance(value_types, list):\n            value_types = [value_types]\n        for value_type in value_types:\n            if not isinstance(value, value_type):\n                value_type_match = False\n                break\n        if not value_type_match:\n            raise ValueError(\"{}config={}, Value type not matched!!! key={}, value={}, value_types={}\".format(\n                log_prefix, config, key, value, value_types))\n    return value\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6e08a1fd1cbab1a840d8f029fa2afed60f67460"},"cell_type":"code","source":"#debug utils\ndef repr_blobs_shape(blobs):\n    res = []\n    for b in blobs:\n        if b is not None: \n            res.append('x'.join(map(str, b.shape)))\n        else:\n            res.append('null')\n    return ','.join(res)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"253f509230f16f4bba8caa9a7e4519ec7892b737"},"cell_type":"code","source":"#log utils\n# -*- coding:utf-8 -*-\nimport os, os.path as osp\nimport time\n\ndef strftime(t = None):\n    return time.strftime(\"%Y%m%d-%H%M%S\", time.localtime(t or time.time()))\n\n#################\n# Logging\n#################\nimport logging\nfrom logging.handlers import TimedRotatingFileHandler\nlogging.basicConfig(format=\"[ %(asctime)s][%(module)s.%(funcName)s] %(message)s\")\n\nDEFAULT_LEVEL = logging.INFO\nDEFAULT_LOGGING_DIR = osp.join(\"logs\", \"gcforest\")\nfh = None\n\ndef init_fh():\n    global fh\n    if fh is not None:\n        return\n    if DEFAULT_LOGGING_DIR is None:\n        return\n    if not osp.exists(DEFAULT_LOGGING_DIR): os.makedirs(DEFAULT_LOGGING_DIR)\n    logging_path = osp.join(DEFAULT_LOGGING_DIR, strftime() + \".log\")\n    fh = logging.FileHandler(logging_path)\n    fh.setFormatter(logging.Formatter(\"[ %(asctime)s][%(module)s.%(funcName)s] %(message)s\"))\n\ndef update_default_level(defalut_level):\n    global DEFAULT_LEVEL\n    DEFAULT_LEVEL = defalut_level\n\ndef update_default_logging_dir(default_logging_dir):\n    global DEFAULT_LOGGING_DIR\n    DEFAULT_LOGGING_DIR = default_logging_dir\n\ndef get_logger(name=\"gcforest\", level=None):\n    level = level or DEFAULT_LEVEL\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    init_fh()\n    if fh is not None:\n        logger.addHandler(fh)\n    return logger\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"66ab70921f8f96d9ad8d4106af2158230695cb98"},"cell_type":"code","source":"#win utils\n# -*- coding:utf-8 -*-\nimport numpy as np\nfrom joblib import Parallel, delayed\n\n#import get_logger\n\nLOGGER = get_logger('win.win_helper')\n\ndef get_windows_channel(X, X_win, des_id, nw, nh, win_x, win_y, stride_x, stride_y):\n    \"\"\"\n    X: N x C x H x W\n    X_win: N x nc x nh x nw\n    (k, di, dj) in range(X.channle, win_y, win_x)\n    \"\"\"\n    #des_id = (k * win_y + di) * win_x + dj\n    dj = des_id % win_x\n    di = des_id / win_x % win_y\n    k = des_id / win_x / win_y\n    src = X[:, k, di:di+nh*stride_y:stride_y, dj:dj+nw*stride_x:stride_x].ravel()\n    des = X_win[des_id, :]\n    np.copyto(des, src)\n\ndef get_windows(X, win_x, win_y, stride_x=1, stride_y=1, pad_x=0, pad_y=0):\n    \"\"\"\n    parallizing get_windows\n    Arguments:\n        X (ndarray): n x c x h x w\n    Return:\n        X_win (ndarray): n x nh x nw x nc\n    \"\"\"\n    assert len(X.shape) == 4\n    n, c, h, w = X.shape\n    if pad_y > 0:\n        X = np.concatenate(( X, np.zeros((n, c, pad_y, w),dtype=X.dtype) ), axis=2)\n        X = np.concatenate(( np.zeros((n, c, pad_y, w),dtype=X.dtype), X ), axis=2)\n    n, c, h, w = X.shape\n    if pad_x > 0:\n        X = np.concatenate(( X, np.zeros((n, c, h, pad_x),dtype=X.dtype) ), axis=3)\n        X = np.concatenate(( np.zeros((n, c, h, pad_x),dtype=X.dtype), X ), axis=3)\n    n, c, h, w = X.shape\n    nc = win_y * win_x * c\n    nh = (h - win_y) / stride_y + 1\n    nw = (w - win_x) / stride_x + 1\n    X_win = np.empty(( nc, n * nh * nw), dtype=np.float32)\n    LOGGER.info(\"get_windows_start: X.shape={}, X_win.shape={}, nw={}, nh={}, c={}, win_x={}, win_y={}, stride_x={}, stride_y={}\".format(\n                X.shape, X_win.shape, nw, nh, c, win_x, win_y, stride_x, stride_y))\n    Parallel(n_jobs=-1, backend=\"threading\", verbose=0)(\n            delayed(get_windows_channel)(X, X_win, des_id, nw, nh, win_x, win_y, stride_x, stride_y)\n            for des_id in range(c * win_x * win_y))\n    LOGGER.info(\"get_windows_end\")\n    X_win = X_win.transpose((1, 0))\n    X_win = X_win.reshape((n, nh, nw, nc))\n    return X_win\n\ndef calc_accuracy(y_gt, y_pred, tag):\n    LOGGER.info(\"Accuracy({})={:.2f}%\".format(tag, np.sum(y_gt==y_pred)*100./len(y_gt)))\n\ndef win_vote(y_win_predict, n_classes):\n    \"\"\" \n     \n    y_win_predict (ndarray): n x n_window\n        y_win_predict[i, j] prediction for the ith data of jth window \n    \"\"\"\n    y_pred = np.zeros(len(y_win_predict), dtype=np.int16)\n    for i, y_bag in enumerate(y_win_predict):\n        y_pred[i] = np.argmax(np.bincount(y_bag,minlength=n_classes))\n    return y_pred\n\ndef win_avg(y_win_proba):\n    \"\"\" \n     \n    Parameters\n    ----------\n    y_win_proba: n x n_windows x n_classes\n    \"\"\"\n    n_classes = y_win_proba.shape[-1]\n    y_bag_proba = np.mean(y_win_proba, axis=1)\n    y_pred = np.argmax(y_bag_proba, axis=1)\n    return y_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"11fb052bebea71d7964fb16e0a1f44985811eee6"},"cell_type":"code","source":"#metrics\n# -*- coding:utf-8 -*-\nimport numpy as np\n\n#from .win_utils import win_vote, win_avg\n\ndef accuracy(y_true, y_pred):\n    return 1.0 * np.sum(y_true == y_pred) / len(y_true)\n\ndef accuracy_pb(y_true, y_proba):\n    y_true = y_true.reshape(-1)\n    y_pred = np.argmax(y_proba.reshape((-1, y_proba.shape[-1])), 1)\n    return 1.0 * np.sum(y_true == y_pred) / len(y_true)\n\ndef accuracy_win_vote(y_true, y_proba):\n    \"\"\"\n \n    \n    Parameters\n    ----------\n    y_true: n x n_windows\n    y_proba: n x n_windows x n_classes\n    \"\"\"\n    n_classes = y_proba.shape[-1]\n    y_pred = win_vote(np.argmax(y_proba, axis=2), n_classes)\n    return accuracy(y_true[:,0], y_pred)\n\ndef accuracy_win_avg(y_true, y_proba):\n    \"\"\"\n \n    \n    Parameters\n    ----------\n    y_true: n x n_windows\n    y_proba: n x n_windows x n_classes\n    \"\"\"\n    y_pred = win_avg(y_proba)\n    return accuracy(y_true[:,0], y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bacd77f8938b000f873df9bec5fbfc032ae0151f"},"cell_type":"code","source":"#base_layer\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport os.path as osp\nimport numpy as np\n\n#from ..utils.log_utils import get_logger\n#from ..utils.config_utils import get_config_value\n\nLOGGER = get_logger('gcforest.layers.base_layer')\n\nclass BaseLayer(object):\n    def __init__(self, layer_config, data_cache):\n        self.layer_config = layer_config\n        self.name = layer_config[\"name\"]\n        self.bottom_names = layer_config[\"bottoms\"]\n        self.top_names = layer_config[\"tops\"]\n        self.data_cache = data_cache\n\n    def get_value(self, key, default_value, value_types, required=False, config=None):\n        return get_config_value(config or self.layer_config, key, default_value, value_types, \n                required=required, config_name=self.name)\n        return value\n\n    def check_top_cache(self, phases, ti):\n        \"\"\"\n        Check if top cache exists\n\n        Parameters\n        ---------\n        phases: List of str\n            e.g. [\"train\", \"test\"]\n        ti: int\n            top index\n\n        Return\n        ------\n        exist_mask: List of bool\n            exist_mask[ti] represent tops[ti] is exist in cache (either keeped in memory or saved in disk)\n        \"\"\"\n        top_name = self.top_names[ti]\n        exist_mask = np.zeros(len(phases))\n        for pi, phase in enumerate(phases):\n            top = self.data_cache.get(phase, top_name, ignore_no_exist=True)\n            exist_mask[pi] = top is not None\n            if top is not None:\n                LOGGER.info(\"[data][{},{}] top cache exists. tops[{}].shape={}\".format(self.name, phase, ti, top.shape))\n        return exist_mask\n\n    def fit_transform(self, train_config):\n        raise NotImplementedError()\n\n    def transform(self):\n        raise NotImplementedError()\n\n    def score(self):\n        pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8dbba9d1d64dea683bc3c65c07520a72d1f52e8e"},"cell_type":"code","source":"#fg_concat_layer\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\n\n# from .base_layer import BaseLayer\n# from ..utils.debug_utils import repr_blobs_shape\n# from ..utils.log_utils import get_logger\n\nLOGGER = get_logger('gcforest.layers.fg_concat_layer')\n\nclass FGConcatLayer(BaseLayer):\n    def __init__(self, layer_config, data_cache):\n        \"\"\"\n        Concat Layer\n        \"\"\"\n        super(FGConcatLayer, self).__init__(layer_config, data_cache)\n        self.axis = self.get_value(\"axis\", -1, int)\n        assert(len(self.bottom_names) > 0)\n        assert(len(self.top_names) == 1)\n\n    def fit_transform(self, train_config):\n        LOGGER.info(\"[data][{}] bottoms={}, tops={}\".format(self.name, self.bottom_names, self.top_names))\n        self._transform(train_config.phases)\n\n    def transform(self):\n        LOGGER.info(\"[data][{}] bottoms={}, tops={}\".format(self.name, self.bottom_names, self.top_names))\n        self._transform([\"test\"])\n\n    def _transform(self, phases):\n        \"\"\"\n        bottoms:\n            for example: n x Ci x w x h\n        \"\"\"\n        for phase in phases:\n            # check top cache\n            if self.check_top_cache([phase], 0)[0]:\n                continue\n            bottoms = self.data_cache.gets(phase, self.bottom_names)\n            LOGGER.info('[data][{},{}] bottoms.shape={}'.format(self.name, phase, repr_blobs_shape(bottoms)))\n            if self.axis == -1:\n                for i, bottom in enumerate(bottoms):\n                    bottoms[i] = bottom.reshape((bottom.shape[0], -1))\n                concat_data = np.concatenate(bottoms, axis=1)\n            else:\n                concat_data = np.concatenate(bottoms, axis=self.axis)\n            LOGGER.info('[data][{},{}] tops[0].shape={}'.format(self.name, phase, concat_data.shape))\n            self.data_cache.update(phase, self.top_names[0], concat_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"415c048fca8d59d71089c7fbbb9b7e3b8d39817f"},"cell_type":"code","source":"#fg pool layer\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\n#from tqdm import trange\n\n# from .base_layer import BaseLayer\n# from ..utils.debug_utils import repr_blobs_shape\n# from ..utils.log_utils import get_logger\n\nLOGGER = get_logger('gcforest.layers.fg_pool_layer')\n\nclass FGPoolLayer(BaseLayer):\n    def __init__(self, layer_config, data_cache):\n        \"\"\"\n        Pooling Layer (MaxPooling, AveragePooling)\n        \"\"\"\n        super(FGPoolLayer, self).__init__(layer_config, data_cache)\n        self.win_x = self.get_value(\"win_x\", None, int, required=True)\n        self.win_y = self.get_value(\"win_y\", None, int, required=True)\n        self.pool_method = self.get_value(\"pool_method\", \"avg\", basestring)\n\n    def fit_transform(self, train_config):\n        LOGGER.info(\"[data][{}] bottoms={}, tops={}\".format(self.name, self.bottom_names, self.top_names))\n        self._transform(train_config.phases, True)\n\n    def transform(self):\n        LOGGER.info(\"[data][{}] bottoms={}, tops={}\".format(self.name, self.bottom_names, self.top_names))\n        self._transform([\"test\"], False)\n\n    def _transform(self, phases, check_top_cache):\n        for ti, top_name in enumerate(self.top_names):\n            LOGGER.info(\"[progress][{}] ti={}/{}, top_name={}\".format(ti, self.name, len(self.top_names), top_name))\n            for phase in phases:\n                # check top cache\n                if check_top_cache and self.check_top_cache([phase], ti)[0]:\n                    continue\n                X = self.data_cache.get(phase, self.bottom_names[ti])\n                LOGGER.info('[data][{},{}] bottoms[{}].shape={}'.format(self.name, phase, ti, X.shape))\n                n, c, h, w = X.shape\n                win_x, win_y = self.win_x, self.win_y\n                #assert h % win_y == 0\n                #assert w % win_x == 0\n                #nh = int(h / win_y)\n                #nw = int(w / win_x)\n                nh = (h - 1) / win_y + 1\n                nw = (w - 1) / win_x + 1\n                X_pool = np.empty(( n, c, nh, nw), dtype=np.float32)\n                #for k in trange(c, desc='loop channel'):\n                #    for di in trange(nh, desc='loop win_y'):\n                #        for dj in trange(nw, desc='loop win_x'):\n                for k in range(c):\n                    for di in range(nh):\n                        for dj in range(nw):\n                            si = di * win_y\n                            sj = dj * win_x\n                            src = X[:, k, si:si+win_y, sj:sj+win_x]\n                            src = src.reshape((X.shape[0], -1))\n                            if self.pool_method == 'max':\n                                X_pool[:, k, di, dj] = np.max(src, axis=1)\n                            elif self.pool_method == 'avg':\n                                X_pool[:, k, di, dj] = np.mean(src, axis=1)\n                            else:\n                                raise ValueError('Unkown Pool Method, pool_method={}'.format(self.pool_method))\n                #print ('\\n')\n                LOGGER.info('[data][{},{}] tops[{}].shape={}'.format(self.name, phase, ti, X_pool.shape))\n                self.data_cache.update(phase, top_name, X_pool)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b017739dfdcac4c4de2b48c08c7fc3bc9582fc3"},"cell_type":"code","source":"#base estimator\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport os, os.path as osp\nimport numpy as np\n\n# from ..utils.log_utils import get_logger\n# from ..utils.cache_utils import name2path\n\nLOGGER = get_logger(\"gcforest.estimators.base_estimator\")\n\ndef check_dir(path):\n    d = osp.abspath(osp.join(path, osp.pardir))\n    if not osp.exists(d):\n        os.makedirs(d)\n\nclass BaseClassifierWrapper(object):\n    def __init__(self, name, est_class, est_args):\n        \"\"\"\n        name: str)\n            Used for debug and as the filename this model may be saved in the disk\n        \"\"\"\n        self.name = name\n        self.est_class = est_class\n        self.est_args = est_args\n        self.cache_suffix = \".pkl\"\n        self.est = None\n\n    def _init_estimator(self):\n        \"\"\"\n        You can re-implement this function when inherient this class\n        \"\"\"\n        est = self.est_class(**self.est_args)\n        return est\n\n    def fit(self, X, y, cache_dir=None):\n        \"\"\"\n        cache_dir(str): \n            if not None\n                then if there is something in cache_dir, dont have fit the thing all over again\n                otherwise, fit it and save to model cache \n        \"\"\"\n        LOGGER.debug(\"X_train.shape={}, y_train.shape={}\".format(X.shape, y.shape))\n        cache_path = self._cache_path(cache_dir)\n        # cache\n        if self._is_cache_exists(cache_path):\n            LOGGER.info(\"Find estimator from {} . skip process\".format(cache_path))\n            return\n        est = self._init_estimator()\n        self._fit(est, X, y)\n        if cache_path is not None:\n            # saved in disk\n            LOGGER.info(\"Save estimator to {} ...\".format(cache_path))\n            check_dir(cache_path); \n            self._save_model_to_disk(est, cache_path)\n        else:\n            # keep in memory\n            self.est = est\n\n    def predict_proba(self, X, cache_dir=None, batch_size=None):\n        LOGGER.debug(\"X.shape={}\".format(X.shape))\n        cache_path = self._cache_path(cache_dir)\n        # cache\n        if cache_path is not None:\n            LOGGER.info(\"Load estimator from {} ...\".format(cache_path))\n            est = self._load_model_from_disk(cache_path)\n            LOGGER.info(\"done ...\")\n        else:\n            est = self.est\n        batch_size = batch_size or self._default_predict_batch_size(est, X)\n        if batch_size > 0:\n            y_proba = self._batch_predict_proba(est, X, batch_size)\n        else:\n            y_proba = self._predict_proba(est, X)\n        LOGGER.debug(\"y_proba.shape={}\".format(y_proba.shape))\n        return y_proba\n\n    def _cache_path(self, cache_dir):\n        if cache_dir is None:\n            return None\n        return osp.join(cache_dir, name2path(self.name) + self.cache_suffix)\n\n    def _is_cache_exists(self, cache_path):\n        return cache_path is not None and osp.exists(cache_path)\n\n    def _batch_predict_proba(self, est, X, batch_size):\n        LOGGER.debug(\"X.shape={}, batch_size={}\".format(X.shape, batch_size))\n        if hasattr(est, \"verbose\"):\n            verbose_backup = est.verbose\n            est.verbose = 0\n        n_datas = X.shape[0]\n        y_pred_proba = None\n        for j in range(0, n_datas, batch_size):\n            LOGGER.info(\"[progress][batch_size={}] ({}/{})\".format(batch_size, j, n_datas))\n            y_cur = self._predict_proba(est, X[j:j+batch_size])\n            if j == 0:\n                n_classes = y_cur.shape[1]\n                y_pred_proba = np.empty((n_datas, n_classes), dtype=np.float32)\n            y_pred_proba[j:j+batch_size,:] = y_cur\n        if hasattr(est, \"verbose\"):\n            est.verbose = verbose_backup\n        return y_pred_proba\n\n    def _load_model_from_disk(self, cache_path):\n        raise NotImplementedError()\n\n    def _save_model_to_disk(self, est, cache_path):\n        raise NotImplementedError()\n\n    def _default_predict_batch_size(self, est, X):\n        \"\"\"\n        You can re-implement this function when inherient this class \n\n        Return\n        ------\n        predict_batch_size (int): default=0\n            if = 0,  predict_proba without batches\n            if > 0, then predict_proba without baches\n            sklearn predict_proba is not so inefficient, has to do this\n        \"\"\"\n        return 0\n\n    def _fit(self, est, X, y):\n        est.fit(X, y)\n\n    def _predict_proba(self, est, X):\n        return est.predict_proba(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93a140b2659fac25a9b11d2e7effbac64f5da318"},"cell_type":"code","source":"#est utils\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\n#from ..utils.log_utils import get_logger\n\nLOGGER = get_logger('gcforest.estimators.est_utils')\n\ndef xgb_train(train_config, X_train, y_train, X_test, y_test):\n    import xgboost as xgb\n    LOGGER.info(\"X_train.shape={}, y_train.shape={}, X_test.shape={}, y_test.shape={}\".format(\n        X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n    param = train_config[\"param\"]\n    xg_train = xgb.DMatrix(X_train, label=y_train)\n    xg_test = xgb.DMatrix(X_test, label=y_test)\n    num_round = int(train_config[\"num_round\"])\n    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n    try:\n        bst = xgb.train(param, xg_train, num_round, watchlist)\n    except KeyboardInterrupt:\n        LOGGER.info(\"Canceld by user's Ctrl-C action\")\n        return\n    y_pred = np.argmax(bst.predict(xg_test), axis=1)\n    acc = 100. * np.sum(y_pred == y_test) / len(y_test)\n    LOGGER.info(\"accuracy={}%\".format(acc))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"009b10187424eb01e0773715188350ca4e1e7aec"},"cell_type":"code","source":"#kfold wrapper\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport os, os.path as osp\nimport numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# from ..utils.log_utils import get_logger\n# from ..utils.cache_utils import name2path\n\nLOGGER = get_logger(\"gcforest.estimators.kfold_wrapper\")\n\nclass KFoldWrapper(object):\n    \"\"\"\n    K-Fold Wrapper\n    \"\"\"\n    def __init__(self, name, n_folds, est_class, est_args, random_state=None):\n        \"\"\"\n        Parameters\n        ----------\n        n_folds (int): \n            Number of folds.\n            If n_folds=1, means no K-Fold\n        est_class (class):\n            Class of estimator\n        est_args (dict):\n            Arguments of estimator\n        random_state (int):\n            random_state used for KFolds split and Estimator\n        \"\"\"\n        self.name = name\n        self.n_folds = n_folds\n        self.est_class = est_class\n        self.est_args = est_args\n        self.random_state = random_state\n        self.estimator1d = [None for k in range(self.n_folds)]\n\n    def _init_estimator(self, k):\n        est_args = self.est_args.copy()\n        est_name = \"{}/{}\".format(self.name, k)\n        est_args[\"random_state\"] = self.random_state\n        return self.est_class(est_name, est_args)\n\n    def fit_transform(self, X, y, y_stratify, cache_dir=None, test_sets=None, eval_metrics=None, keep_model_in_mem=True):\n        \"\"\"\n        X (ndarray):\n            n x k or n1 x n2 x k\n            to support windows_layer, X could have dim >2 \n        y (ndarray):\n            n or n1 x n2\n        y_stratify (list):\n            used for StratifiedKFold or None means no stratify\n        test_sets (list): optional\n            A list of (prefix, X_test, y_test) pairs.\n            predict_proba for X_test will be returned \n            use with keep_model_in_mem=False to save mem useage\n            y_test could be None, otherwise use eval_metrics for debugging\n        eval_metrics (list): optional\n            A list of (str, callable functions)\n        keep_model_in_mem (bool):\n        \"\"\"\n        if cache_dir is not None:\n            cache_dir = osp.join(cache_dir, name2path(self.name))\n        assert 2 <= len(X.shape) <= 3, \"X.shape should be n x k or n x n2 x k\"\n        assert len(X.shape) == len(y.shape) + 1\n        assert X.shape[0] == len(y_stratify)\n        test_sets = test_sets if test_sets is not None else []\n        eval_metrics = eval_metrics if eval_metrics is not None else []\n        # K-Fold split\n        n_stratify = X.shape[0]\n        if self.n_folds == 1:\n            cv = [(range(len(X)), range(len(X)))]\n        else:\n            if y_stratify is None:\n                skf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n                cv = [(t, v) for (t, v) in skf.split(len(n_stratify))]\n            else:\n                skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n                cv = [(t, v) for (t, v) in skf.split(range(n_stratify), y_stratify)]\n        # Fit\n        y_probas = []\n        n_dims = X.shape[-1]\n        n_datas = X.size / n_dims\n        inverse = False\n        for k in range(self.n_folds):\n            est = self._init_estimator(k)\n            if not inverse:\n                train_idx, val_idx = cv[k]\n            else:\n                val_idx, train_idx = cv[k]\n            # fit on k-fold train\n            est.fit(X[train_idx].reshape((-1, n_dims)), y[train_idx].reshape(-1), cache_dir=cache_dir)\n\n            # predict on k-fold validation\n            y_proba = est.predict_proba(X[val_idx].reshape((-1, n_dims)), cache_dir=cache_dir)\n            if len(X.shape) == 3:\n                y_proba = y_proba.reshape((len(val_idx), -1, y_proba.shape[-1]))\n            self.log_eval_metrics(self.name, y[val_idx], y_proba, eval_metrics, \"train_{}\".format(k))\n\n            # merging result\n            if k == 0:\n                if len(X.shape) == 2:\n                    y_proba_cv = np.zeros((n_stratify, y_proba.shape[1]), dtype=np.float32)\n                else:\n                    y_proba_cv = np.zeros((n_stratify, y_proba.shape[1], y_proba.shape[2]), dtype=np.float32)\n                y_probas.append(y_proba_cv)\n            y_probas[0][val_idx, :] += y_proba\n            if keep_model_in_mem:\n                self.estimator1d[k] = est\n\n            # test\n            for vi, (prefix, X_test, y_test) in enumerate(test_sets):\n                y_proba = est.predict_proba(X_test.reshape((-1, n_dims)), cache_dir=cache_dir)\n                if len(X.shape) == 3:\n                    y_proba = y_proba.reshape((X_test.shape[0], X_test.shape[1], y_proba.shape[-1]))\n                if k == 0:\n                    y_probas.append(y_proba)\n                else:\n                    y_probas[vi + 1] += y_proba\n        if inverse and self.n_folds > 1:\n            y_probas[0] /= (self.n_folds - 1)\n        for y_proba in y_probas[1:]:\n            y_proba /= self.n_folds\n        # log\n        self.log_eval_metrics(self.name, y, y_probas[0], eval_metrics, \"train_cv\")\n        for vi, (test_name, X_test, y_test) in enumerate(test_sets):\n            if y_test is not None:\n                self.log_eval_metrics(self.name, y_test, y_probas[vi + 1], eval_metrics, test_name)\n        return y_probas\n\n    def log_eval_metrics(self, est_name, y_true, y_proba, eval_metrics, y_name):\n        \"\"\"\n        y_true (ndarray): n or n1 x n2\n        y_proba (ndarray): n x n_classes or n1 x n2 x n_classes\n        \"\"\"\n        if eval_metrics is None:\n            return\n        for (eval_name, eval_metric) in eval_metrics:\n            accuracy = eval_metric(y_true, y_proba)\n            LOGGER.info(\"Accuracy({}.{}.{})={:.2f}%\".format(est_name, y_name, eval_name, accuracy * 100.))\n\n    def predict_proba(self, X_test):\n        assert 2 <= len(X_test.shape) <= 3, \"X_test.shape should be n x k or n x n2 x k\"\n        # K-Fold split\n        n_dims = X_test.shape[-1]\n        n_datas = X_test.size / n_dims\n        for k in range(self.n_folds):\n            est = self.estimator1d[k]\n            y_proba = est.predict_proba(X_test.reshape((-1, n_dims)), cache_dir=None)\n            if len(X_test.shape) == 3:\n                y_proba = y_proba.reshape((X_test.shape[0], X_test.shape[1], y_proba.shape[-1]))\n            if k == 0:\n                y_proba_kfolds = y_proba\n            else:\n                y_proba_kfolds += y_proba\n        y_proba_kfolds /= self.n_folds\n        return y_proba_kfolds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"81d9e2ba44850e5c7be7382d66b8d4587a0ff097"},"cell_type":"code","source":"#sklearn estimators\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nfrom sklearn.externals import joblib\n\n# from .base_estimator import BaseClassifierWrapper\n# from ..utils.log_utils import get_logger\n\nLOGGER = get_logger('gcforest.estimators.sklearn_estimators')\n\ndef forest_predict_batch_size(clf, X):\n    import psutil\n    free_memory = psutil.virtual_memory().free\n    if free_memory < 2e9:\n        free_memory = int(2e9)\n    max_mem_size = max(int(free_memory * 0.5), int(8e10))\n    mem_size_1 = clf.n_classes_ * clf.n_estimators * 16\n    batch_size = (max_mem_size - 1) / mem_size_1 + 1\n    if batch_size < 10:\n        batch_size = 10\n    if batch_size >= X.shape[0]:\n        return 0\n    return batch_size\n\nclass SKlearnBaseClassifier(BaseClassifierWrapper):\n    def _load_model_from_disk(self, cache_path):\n        return joblib.load(cache_path)\n\n    def _save_model_to_disk(self, clf, cache_path):\n        joblib.dump(clf, cache_path)\n\nclass GCExtraTreesClassifier(SKlearnBaseClassifier):\n    def __init__(self, name, kwargs):\n        from sklearn.ensemble import ExtraTreesClassifier\n        super(GCExtraTreesClassifier, self).__init__(name, ExtraTreesClassifier, kwargs)\n    \n    def _default_predict_batch_size(self, clf, X):\n        return forest_predict_batch_size(clf, X)\n\nclass GCRandomForestClassifier(SKlearnBaseClassifier):\n    def __init__(self, name, kwargs):\n        from sklearn.ensemble import RandomForestClassifier\n        super(GCRandomForestClassifier, self).__init__(name, RandomForestClassifier, kwargs)\n    \n    def _default_predict_batch_size(self, clf, X):\n        return forest_predict_batch_size(clf, X)\n\n\nclass GCLR(SKlearnBaseClassifier):\n    def __init__(self,name,kwargs):\n        from sklearn.linear_model import LogisticRegression\n        super(GCLR,self).__init__(name,LogisticRegression,kwargs)\n\n\nclass GCSGDClassifier(SKlearnBaseClassifier):\n    def __init__(self,name,kwargs):\n        from sklearn.linear_model import SGDClassifier\n        super(GCSGDClassifier,self).__init__(name,SGDClassifier,kwargs)\n\n\nclass GCXGBClassifier(SKlearnBaseClassifier):\n    def __init__(self,name,kwargs):\n        import xgboost as xgb\n        kwargs = kwargs.copy()\n        if \"random_state\" in kwargs:\n            kwargs[\"seed\"] = kwargs[\"random_state\"]\n            kwargs.pop(\"random_state\")\n        super(GCXGBClassifier,self).__init__(name,xgb.XGBClassifier,kwargs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63b3d6054883f14f7bbd902d35e3ded5d8b57e6c"},"cell_type":"code","source":"#estimator init\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\n# from .base_estimator import BaseClassifierWrapper\n# from .sklearn_estimators import GCSGDClassifier,GCLR, GCExtraTreesClassifier, GCRandomForestClassifier, GCXGBClassifier\n# #from .xgb_estimator import GCXGBClassifier\n# from .kfold_wrapper import KFoldWrapper\n\ndef get_estimator_class(est_type):\n    if est_type == \"ExtraTreesClassifier\":\n        return GCExtraTreesClassifier\n    if est_type == \"RandomForestClassifier\":\n        return GCRandomForestClassifier\n    if est_type == \"LogisticRegression\":\n        return GCLR\n    if est_type == \"SGDClassifier\":\n        return GCSGDClassifier\n    if est_type == \"XGBClassifier\":\n        return GCXGBClassifier\n    #if est_type == \"XGBClassifier\":\n    #    return GCXGBClassifier\n    raise ValueError('Unkown Estimator Type, est_type={}'.format(est_type))\n\ndef get_estimator(name, est_type, est_args):\n    est_class = get_estimator_class(est_type)\n    return est_class(name, est_args)\n\ndef get_estimator_kfold(name, n_splits, est_type, est_args, random_state=None):\n    est_class = get_estimator_class(est_type)\n    return KFoldWrapper(name, n_splits, est_class, est_args, random_state=random_state)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c77211846bcee1ff6e964915283a24f2253a649b"},"cell_type":"code","source":"#fg win layer\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\n\n# from .base_layer import BaseLayer\n# from ..estimators import get_estimator_kfold\n# from ..utils.metrics import accuracy_pb, accuracy_win_vote, accuracy_win_avg\n# from ..utils.win_utils import get_windows\n# from ..utils.debug_utils import repr_blobs_shape\n# from ..utils.log_utils import get_logger\n\nLOGGER = get_logger(\"gcforest.layers.fg_win_layer\")\n\n#CV_POLICYS = [\"data\", \"win\"]\n#CV_POLICYS = [\"data\"]\n\nclass FGWinLayer(BaseLayer):\n    def __init__(self, layer_config, data_cache):\n        \"\"\"\n        est_config (dict): \n            estimator的config\n        win_x, win_y, stride_x, stride_y, pad_x, pad_y (int): \n            configs for windows \n        n_folds(int): default=1\n             1 means do not use k-fold\n        n_classes (int):\n             \n        \"\"\"\n        super(FGWinLayer, self).__init__(layer_config, data_cache)\n        # estimator\n        self.est_configs = self.get_value(\"estimators\", None, list, required=True)\n        self.win_x = self.get_value(\"win_x\", None, int, required=True)\n        self.win_y = self.get_value(\"win_y\", None, int, required=True)\n        self.stride_x = self.get_value(\"stride_x\", 1, int)\n        self.stride_y = self.get_value(\"stride_y\", 1, int)\n        self.pad_x = self.get_value(\"pad_x\", 0, int)\n        self.pad_y = self.get_value(\"pad_y\", 0, int)\n        self.n_classes = self.get_value(\"n_classes\", None, int, required=True)\n        #self.cv_policy = layer_config.get(\"cv_policy\", \"data\")\n        #assert(self.cv_policy in CV_POLICYS)\n        assert len(self.bottom_names) >= 2\n        assert len(self.est_configs) == len(self.top_names), \"Each estimator shoud produce one unique top\"\n        # self.eval_metrics = [(\"predict\", accuracy_pb), (\"vote\", accuracy_win_vote), (\"avg\", accuracy_win_avg)]\n        self.eval_metrics = [(\"predict\", accuracy_pb), (\"avg\", accuracy_win_avg)]\n        self.estimator1d = [None for ei in range(len(self.est_configs))]\n\n    def _init_estimators(self, ei, random_state):\n        \"\"\"\n        ei (int): estimator index\n        \"\"\"\n        top_name = self.top_names[ei]\n        est_args = self.est_configs[ei].copy()\n        est_name =\"{}/{}_folds\".format(top_name, est_args[\"n_folds\"])\n        # n_folds\n        n_folds = int(est_args[\"n_folds\"])\n        est_args.pop(\"n_folds\")\n        # est_type\n        est_type = est_args[\"type\"]\n        est_args.pop(\"type\")\n        # random_state\n        random_state = (random_state + hash(\"[estimator] {}\".format(est_name))) % 1000000007\n        return get_estimator_kfold(est_name, n_folds, est_type, est_args, random_state=random_state)\n\n    def fit_transform(self, train_config):\n        LOGGER.info(\"[data][{}], bottoms={}, tops={}\".format(self.name, self.bottom_names, self.top_names))\n        phases = train_config.phases\n        X_train_win, y_train_win = None, None\n        test_sets = None\n\n        for ti, top_name in enumerate(self.top_names):\n            LOGGER.info(\"[progress][{}] ti={}/{}, top_name={}\".format(self.name, ti, len(self.top_names), top_name))\n            # check top cache\n            if np.all(self.check_top_cache(phases, ti)):\n                LOGGER.info(\"[data][{}] all top cache exists. skip progress\".format(self.name))\n                continue\n\n            # init X, y, n_classes\n            if X_train_win is None:\n                for pi, phase in enumerate(phases):\n                    bottoms = self.data_cache.gets(phase, self.bottom_names)\n                    LOGGER.info('[data][{},{}] bottoms.shape={}'.format(self.name, phase, repr_blobs_shape(bottoms)))\n                    X, y = np.concatenate(bottoms[:-1], axis=1), bottoms[-1]\n                    # n x n_windows x channel\n                    X_win = get_windows(X, self.win_x, self.win_y, self.stride_x, self.stride_y, self.pad_x, self.pad_y)\n                    _, nh, nw, _ = X_win.shape\n                    X_win = X_win.reshape((X_win.shape[0], -1, X_win.shape[-1]))\n                    y_win = y[:,np.newaxis].repeat(X_win.shape[1], axis=1)\n                    if pi == 0:\n                        assert self.n_classes == len(np.unique(y)), \\\n                                \"n_classes={}, len(unique(y))={}\".format(self.n_classes, len(np.unique(y)))\n                        X_train_win, y_train_win = X_win, y_win\n                    else:\n                        test_sets = [(\"test\", X_win, y_win), ]\n\n            # fit\n            est = self._init_estimators(ti, train_config.random_state)\n            y_probas = est.fit_transform(X_train_win, y_train_win, y_train_win[:,0], cache_dir=train_config.model_cache_dir, \n                    test_sets = test_sets, eval_metrics=self.eval_metrics,\n                    keep_model_in_mem=train_config.keep_model_in_mem)\n\n            for pi, phase in enumerate(phases):\n                y_proba = y_probas[pi].reshape((-1, nh, nw, self.n_classes)).transpose((0, 3, 1, 2))\n                LOGGER.info('[data][{},{}] tops[{}].shape={}'.format(self.name, phase, ti, y_proba.shape))\n                self.data_cache.update(phase, self.top_names[ti], y_proba)\n            if train_config.keep_model_in_mem:\n                self.estimator1d[ti] = est\n    \n    def transform(self):\n        phase = \"test\"\n        for ti, top_name in enumerate(self.top_names):\n            LOGGER.info(\"[progress][{}] ti={}/{}, top_name={}\".format(self.name, ti, len(self.top_names), top_name))\n\n            bottoms = self.data_cache.gets(phase, self.bottom_names[:-1])\n            LOGGER.info('[data][{},{}] bottoms.shape={}'.format(self.name, phase, repr_blobs_shape(bottoms)))\n            X = np.concatenate(bottoms, axis=1)\n            # n x n_windows x channel\n            X_win = get_windows(X, self.win_x, self.win_y, self.stride_x, self.stride_y, self.pad_x, self.pad_y)\n            _, nh, nw, _ = X_win.shape\n            X_win = X_win.reshape((X_win.shape[0], -1, X_win.shape[-1]))\n\n            est = self.estimator1d[ti]\n            y_proba = est.predict_proba(X_win)\n            y_proba = y_proba.reshape((-1, nh, nw, self.n_classes)).transpose((0, 3, 1, 2))\n            LOGGER.info('[data][{},{}] tops[{}].shape={}'.format(self.name, phase, ti, y_proba.shape))\n            self.data_cache.update(phase, self.top_names[ti], y_proba)\n\n    def score(self):\n        eval_metrics = [(\"predict\", accuracy_pb), (\"avg\", accuracy_win_avg)]\n        for ti, top_name in enumerate(self.top_names):\n            for phase in [\"train\", \"test\"]:\n                y = self.data_cache.get(phase, self.bottom_names[-1])\n                y_proba = self.data_cache.get(phase, top_name)\n                y_proba = y_proba.transpose((0,2,3,1))\n                y_proba = y_proba.reshape((y_proba.shape[0], -1, y_proba.shape[3]))\n                y = y[:,np.newaxis].repeat(y_proba.shape[1], axis=1)\n                for eval_name, eval_metric in eval_metrics:\n                    acc = eval_metric(y, y_proba)\n                    LOGGER.info(\"Accuracy({}.{}.{})={:.2f}%\".format(top_name, phase, eval_name, acc*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52d8ea8699726fca088b27c90871fd6bd3d7e437"},"cell_type":"code","source":"#layers init\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\n# from .base_layer import BaseLayer\n# from .fg_concat_layer import FGConcatLayer\n# from .fg_pool_layer import FGPoolLayer\n# from .fg_win_layer import FGWinLayer\n\ndef get_layer_class(layer_type):\n    if layer_type == \"FGWinLayer\":\n        return FGWinLayer\n    if layer_type == \"FGConcatLayer\":\n        return FGConcatLayer\n    if layer_type == \"FGPoolLayer\":\n        return FGPoolLayer\n    raise ValueError(\"Unkown Layer Type: \", layer_type)\n\ndef get_layer(layer_config, data_cache):\n    \"\"\"\n    layer_config (dict): config for layer \n    data_cache (gcforest.DataCache): DataCache \n    \"\"\"\n    layer_config = layer_config.copy()\n    layer_class = get_layer_class(layer_config[\"type\"])\n    layer_config.pop(\"type\")\n    layer = layer_class(layer_config, data_cache)\n    return layer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"12f6d6eb98f50fe347f7c1faa8053469b6b10126"},"cell_type":"code","source":"#cascade classifier\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets.\nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng.\n\"\"\"\nimport numpy as np\nimport os\nimport os.path as osp\nimport pickle\n\n# from ..estimators import get_estimator_kfold\n# from ..utils.config_utils import get_config_value\n# from ..utils.log_utils import get_logger\n# from ..utils.metrics import accuracy_pb\n\nLOGGER = get_logger('gcforest.cascade.cascade_classifier')\n\n\ndef check_dir(path):\n    d = osp.abspath(osp.join(path, osp.pardir))\n    if not osp.exists(d):\n        os.makedirs(d)\n\n\ndef calc_accuracy(y_true, y_pred, name, prefix=\"\"):\n    acc = 100. * np.sum(np.asarray(y_true) == y_pred) / len(y_true)\n    LOGGER.info('{}Accuracy({})={:.2f}%'.format(prefix, name, acc))\n    return acc\n\n\ndef get_opt_layer_id(acc_list):\n    \"\"\" Return layer id with max accuracy on training data \"\"\"\n    opt_layer_id = np.argsort(-np.asarray(acc_list), kind='mergesort')[0]\n    return opt_layer_id\n\n\nclass CascadeClassifier(object):\n    def __init__(self, ca_config):\n        \"\"\"\n        Parameters (ca_config)\n        ----------\n        early_stopping_rounds: int\n            when not None , means when the accuracy does not increase in early_stopping_rounds, the cascade level will stop automatically growing\n        max_layers: int\n            maximum number of cascade layers allowed for exepriments, 0 means use Early Stoping to automatically find the layer number\n        n_classes: int\n            Number of classes\n        est_configs:\n            List of CVEstimator's config\n        look_indexs_cycle (list 2d): default=None\n            specification for layer i, look for the array in look_indexs_cycle[i % len(look_indexs_cycle)]\n            defalut = None <=> [range(n_groups)]\n            .e.g.\n                look_indexs_cycle = [[0,1],[2,3],[0,1,2,3]]\n                means layer 1 look for the grained 0,1; layer 2 look for grained 2,3; layer 3 look for every grained, and layer 4 cycles back as layer 1\n        data_save_rounds: int [default=0]\n        data_save_dir: str [default=None]\n            each data_save_rounds save the intermidiate results in data_save_dir\n            if data_save_rounds = 0, then no savings for intermidiate results\n        \"\"\"\n        self.ca_config = ca_config\n        self.early_stopping_rounds = self.get_value(\"early_stopping_rounds\", None, int, required=True)\n        self.max_layers = self.get_value(\"max_layers\", 0, int)\n        self.n_classes = self.get_value(\"n_classes\", None, int, required=True)\n        self.est_configs = self.get_value(\"estimators\", None, list, required=True)\n        self.look_indexs_cycle = self.get_value(\"look_indexs_cycle\", None, list)\n        self.random_state = self.get_value(\"random_state\", None, int)\n        # self.data_save_dir = self.get_value(\"data_save_dir\", None, basestring)\n        self.data_save_dir = ca_config.get(\"data_save_dir\", None)\n        self.data_save_rounds = self.get_value(\"data_save_rounds\", 0, int)\n        if self.data_save_rounds > 0:\n            assert self.data_save_dir is not None, \"data_save_dir should not be null when data_save_rounds>0\"\n        self.eval_metrics = [(\"predict\", accuracy_pb)]\n        self.estimator2d = {}\n        self.opt_layer_num = -1\n        # LOGGER.info(\"\\n\" + json.dumps(ca_config, sort_keys=True, indent=4, separators=(',', ':')))\n\n    @property\n    def n_estimators_1(self):\n        # estimators of one layer\n        return len(self.est_configs)\n\n    def get_value(self, key, default_value, value_types, required=False):\n        return get_config_value(self.ca_config, key, default_value, value_types,\n                required=required, config_name=\"cascade\")\n\n    def _set_estimator(self, li, ei, est):\n        if li not in self.estimator2d:\n            self.estimator2d[li] = {}\n        self.estimator2d[li][ei] = est\n\n    def _get_estimator(self, li, ei):\n        return self.estimator2d.get(li, {}).get(ei, None)\n\n    def _init_estimators(self, li, ei):\n        est_args = self.est_configs[ei].copy()\n        est_name = \"layer_{} - estimator_{} - {}_folds\".format(li, ei, est_args[\"n_folds\"])\n        # n_folds\n        n_folds = int(est_args[\"n_folds\"])\n        est_args.pop(\"n_folds\")\n        # est_type\n        est_type = est_args[\"type\"]\n        est_args.pop(\"type\")\n        # random_state\n        if self.random_state is not None:\n            random_state = (self.random_state + hash(\"[estimator] {}\".format(est_name))) % 1000000007\n        else:\n            random_state = None\n        return get_estimator_kfold(est_name, n_folds, est_type, est_args, random_state=random_state)\n\n    def _check_look_indexs_cycle(self, X_groups, is_fit):\n        # check look_indexs_cycle\n        n_groups = len(X_groups)\n        if is_fit and self.look_indexs_cycle is None:\n            look_indexs_cycle = [list(range(n_groups))]\n        else:\n            look_indexs_cycle = self.look_indexs_cycle\n            for look_indexs in look_indexs_cycle:\n                if np.max(look_indexs) >= n_groups or np.min(look_indexs) < 0 or len(look_indexs) == 0:\n                    raise ValueError(\"look_indexs doesn't match n_groups!!! look_indexs={}, n_groups={}\".format(\n                        look_indexs, n_groups))\n        if is_fit:\n            self.look_indexs_cycle = look_indexs_cycle\n        return look_indexs_cycle\n\n    def _check_group_dims(self, X_groups, is_fit):\n        if is_fit:\n            group_starts, group_ends, group_dims = [], [], []\n        else:\n            group_starts, group_ends, group_dims = self.group_starts, self.group_ends, self.group_dims\n        n_datas = X_groups[0].shape[0]\n        X = np.zeros((n_datas, 0), dtype=X_groups[0].dtype)\n        for i, X_group in enumerate(X_groups):\n            assert(X_group.shape[0] == n_datas)\n            X_group = X_group.reshape(n_datas, -1)\n            if is_fit:\n                group_dims.append( X_group.shape[1] )\n                group_starts.append(0 if i == 0 else group_ends[i - 1])\n                group_ends.append(group_starts[i] + group_dims[i])\n            else:\n                assert(X_group.shape[1] == group_dims[i])\n            X = np.hstack((X, X_group))\n        if is_fit:\n            self.group_starts, self.group_ends, self.group_dims = group_starts, group_ends, group_dims\n        return group_starts, group_ends, group_dims, X\n\n    def fit_transform(self, X_groups_train, y_train, X_groups_test, y_test, stop_by_test=False, train_config=None):\n        \"\"\"\n        fit until the accuracy converges in early_stop_rounds\n        stop_by_test: (bool)\n            When X_test, y_test is validation data that used for determine the opt_layer_id,\n            use this option\n        \"\"\"\n        if train_config is None:\n            from ..config import GCTrainConfig\n            train_config = GCTrainConfig({})\n        data_save_dir = train_config.data_cache.cache_dir or self.data_save_dir\n\n        is_eval_test = \"test\" in train_config.phases\n        if not type(X_groups_train) == list:\n            X_groups_train = [X_groups_train]\n        if is_eval_test and not type(X_groups_test) == list:\n            X_groups_test = [X_groups_test]\n        LOGGER.info(\"X_groups_train.shape={},y_train.shape={},X_groups_test.shape={},y_test.shape={}\".format(\n            [xr.shape for xr in X_groups_train], y_train.shape,\n            [xt.shape for xt in X_groups_test] if is_eval_test else \"no_test\", y_test.shape if is_eval_test else \"no_test\"))\n\n        # check look_indexs_cycle\n        look_indexs_cycle = self._check_look_indexs_cycle(X_groups_train, True)\n        if is_eval_test:\n            self._check_look_indexs_cycle(X_groups_test, False)\n\n        # check groups dimension\n        group_starts, group_ends, group_dims, X_train = self._check_group_dims(X_groups_train, True)\n        if is_eval_test:\n            _, _, _, X_test = self._check_group_dims(X_groups_test, False)\n        else:\n            X_test = np.zeros((0, X_train.shape[1]))\n        LOGGER.info(\"group_dims={}\".format(group_dims))\n        LOGGER.info(\"group_starts={}\".format(group_starts))\n        LOGGER.info(\"group_ends={}\".format(group_ends))\n        LOGGER.info(\"X_train.shape={},X_test.shape={}\".format(X_train.shape, X_test.shape))\n\n        n_trains = X_groups_train[0].shape[0]\n        n_tests = X_groups_test[0].shape[0] if is_eval_test else 0\n\n        n_classes = self.n_classes\n        assert n_classes == len(np.unique(y_train)), \"n_classes({}) != len(unique(y)) {}\".format(n_classes, np.unique(y_train))\n        train_acc_list = []\n        test_acc_list = []\n        # X_train, y_train, X_test, y_test\n        opt_datas = [None, None, None, None]\n        try:\n            # probability of each cascades's estimators\n            X_proba_train = np.zeros((n_trains, n_classes * self.n_estimators_1), dtype=np.float32)\n            X_proba_test = np.zeros((n_tests, n_classes * self.n_estimators_1), dtype=np.float32)\n            X_cur_train, X_cur_test = None, None\n            layer_id = 0\n            while 1:\n                if self.max_layers > 0 and layer_id >= self.max_layers:\n                    break\n                # Copy previous cascades's probability into current X_cur\n                if layer_id == 0:\n                    # first layer not have probability distribution\n                    X_cur_train = np.zeros((n_trains, 0), dtype=np.float32)\n                    X_cur_test = np.zeros((n_tests, 0), dtype=np.float32)\n                else:\n                    X_cur_train = X_proba_train.copy()\n                    X_cur_test = X_proba_test.copy()\n                # Stack data that current layer needs in to X_cur\n                look_indexs = look_indexs_cycle[layer_id % len(look_indexs_cycle)]\n                for _i, i in enumerate(look_indexs):\n                    X_cur_train = np.hstack((X_cur_train, X_train[:, group_starts[i]:group_ends[i]]))\n                    X_cur_test = np.hstack((X_cur_test, X_test[:, group_starts[i]:group_ends[i]]))\n                LOGGER.info(\"[layer={}] look_indexs={}, X_cur_train.shape={}, X_cur_test.shape={}\".format(\n                    layer_id, look_indexs, X_cur_train.shape, X_cur_test.shape))\n                # Fit on X_cur, predict to update X_proba\n                y_train_proba_li = np.zeros((n_trains, n_classes))\n                y_test_proba_li = np.zeros((n_tests, n_classes))\n                for ei, est_config in enumerate(self.est_configs):\n                    est = self._init_estimators(layer_id, ei)\n                    # fit_trainsform\n                    test_sets = [(\"test\", X_cur_test, y_test)] if n_tests > 0 else None\n                    y_probas = est.fit_transform(X_cur_train, y_train, y_train,\n                            test_sets=test_sets, eval_metrics=self.eval_metrics,\n                            keep_model_in_mem=train_config.keep_model_in_mem)\n                    # train\n                    X_proba_train[:, ei * n_classes: ei * n_classes + n_classes] = y_probas[0]\n                    y_train_proba_li += y_probas[0]\n                    # test\n                    if n_tests > 0:\n                        X_proba_test[:, ei * n_classes: ei * n_classes + n_classes] = y_probas[1]\n                        y_test_proba_li += y_probas[1]\n                    if train_config.keep_model_in_mem:\n                        self._set_estimator(layer_id, ei, est)\n                y_train_proba_li /= len(self.est_configs)\n                train_avg_acc = calc_accuracy(y_train, np.argmax(y_train_proba_li, axis=1), 'layer_{} - train.classifier_average'.format(layer_id))\n                train_acc_list.append(train_avg_acc)\n                if n_tests > 0:\n                    y_test_proba_li /= len(self.est_configs)\n                    test_avg_acc = calc_accuracy(y_test, np.argmax(y_test_proba_li, axis=1), 'layer_{} - test.classifier_average'.format(layer_id))\n                    test_acc_list.append(test_avg_acc)\n                else:\n                    test_acc_list.append(0.0)\n\n                opt_layer_id = get_opt_layer_id(test_acc_list if stop_by_test else train_acc_list)\n                # set opt_datas\n                if opt_layer_id == layer_id:\n                    opt_datas = [X_proba_train, y_train, X_proba_test if n_tests > 0 else None, y_test]\n                # early stop\n                if self.early_stopping_rounds > 0 and layer_id - opt_layer_id >= self.early_stopping_rounds:\n                    # log and save final result (opt layer)\n                    LOGGER.info(\"[Result][Optimal Level Detected] opt_layer_num={}, accuracy_train={:.2f}%, accuracy_test={:.2f}%\".format(\n                        opt_layer_id + 1, train_acc_list[opt_layer_id], test_acc_list[opt_layer_id]))\n                    if data_save_dir is not None:\n                        self.save_data( data_save_dir, opt_layer_id, *opt_datas)\n                    # remove unused model\n                    if train_config.keep_model_in_mem:\n                        for li in range(opt_layer_id + 1, layer_id + 1):\n                            for ei, est_config in enumerate(self.est_configs):\n                                self._set_estimator(li, ei, None)\n                    self.opt_layer_num = opt_layer_id + 1\n                    return opt_layer_id, opt_datas[0], opt_datas[1], opt_datas[2], opt_datas[3]\n                # save opt data if needed\n                if self.data_save_rounds > 0 and (layer_id + 1) % self.data_save_rounds == 0:\n                    self.save_data(data_save_dir, layer_id, *opt_datas)\n                # inc layer_id\n                layer_id += 1\n            LOGGER.info(\"[Result][Reach Max Layer] opt_layer_num={}, accuracy_train={:.2f}%, accuracy_test={:.2f}%\".format(\n                opt_layer_id + 1, train_acc_list[opt_layer_id], test_acc_list[opt_layer_id]))\n            if data_save_dir is not None:\n                self.save_data(data_save_dir, self.max_layers - 1, *opt_datas)\n            self.opt_layer_num = self.max_layers\n            return self.max_layers, opt_datas[0], opt_datas[1], opt_datas[2], opt_datas[3]\n        except KeyboardInterrupt:\n            pass\n\n    def transform(self, X_groups_test):\n        if not type(X_groups_test) == list:\n            X_groups_test = [X_groups_test]\n        LOGGER.info(\"X_groups_test.shape={}\".format([xt.shape for xt in X_groups_test]))\n        # check look_indexs_cycle\n        look_indexs_cycle = self._check_look_indexs_cycle(X_groups_test, False)\n        # check group_dims\n        group_starts, group_ends, group_dims, X_test = self._check_group_dims(X_groups_test, False)\n        LOGGER.info(\"group_dims={}\".format(group_dims))\n        LOGGER.info(\"X_test.shape={}\".format(X_test.shape))\n\n        n_tests = X_groups_test[0].shape[0]\n        n_classes = self.n_classes\n\n        # probability of each cascades's estimators\n        X_proba_test = np.zeros((X_test.shape[0], n_classes * self.n_estimators_1), dtype=np.float32)\n        X_cur_test = None\n        for layer_id in range(self.opt_layer_num):\n            # Copy previous cascades's probability into current X_cur\n            if layer_id == 0:\n                # first layer not have probability distribution\n                X_cur_test = np.zeros((n_tests, 0), dtype=np.float32)\n            else:\n                X_cur_test = X_proba_test.copy()\n            # Stack data that current layer needs in to X_cur\n            look_indexs = look_indexs_cycle[layer_id % len(look_indexs_cycle)]\n            for _i, i in enumerate(look_indexs):\n                X_cur_test = np.hstack((X_cur_test, X_test[:, group_starts[i]:group_ends[i]]))\n            LOGGER.info(\"[layer={}] look_indexs={}, X_cur_test.shape={}\".format(\n                layer_id, look_indexs, X_cur_test.shape))\n            for ei, est_config in enumerate(self.est_configs):\n                est = self._get_estimator(layer_id, ei)\n                if est is None:\n                    raise ValueError(\"model (li={}, ei={}) not present, maybe you should set keep_model_in_mem to True\".format(\n                        layer_id, ei))\n                y_probas = est.predict_proba(X_cur_test)\n                X_proba_test[:, ei * n_classes:ei * n_classes + n_classes] = y_probas\n        return X_proba_test\n\n    def predict_proba(self, X):\n        # n x (n_est*n_classes)\n        y_proba = self.transform(X)\n        # n x n_est x n_classes\n        y_proba = y_proba.reshape((y_proba.shape[0], self.n_estimators_1, self.n_classes))\n        y_proba = y_proba.mean(axis=1)\n        return y_proba\n\n    def save_data(self, data_save_dir, layer_id, X_train, y_train, X_test, y_test):\n        for pi, phase in enumerate([\"train\", \"test\"]):\n            data_path = osp.join(data_save_dir, \"layer_{}-{}.pkl\".format(layer_id, phase))\n            check_dir(data_path)\n            data = {\"X\": X_train, \"y\": y_train} if pi == 0 else {\"X\": X_test, \"y\": y_test}\n            LOGGER.info(\"Saving Data in {} ... X.shape={}, y.shape={}\".format(data_path, data[\"X\"].shape, data[\"y\"].shape))\n            with open(data_path, \"wb\") as f:\n                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4562f66f16cb8a0a6a2038dc792b9b2f15633f84"},"cell_type":"code","source":"#data cache\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport os, os.path as osp\nimport numpy as np\n\n# from .utils.log_utils import get_logger\n# from .utils.cache_utils import name2path\n\nLOGGER = get_logger(\"gcforest.data_cache\")\n\ndef check_dir(path):\n    \"\"\" make sure the dir specified by path got created \"\"\"\n    d = osp.abspath(osp.join(path, osp.pardir))\n    if not osp.exists(d):\n        os.makedirs(d)\n\ndef data_disk_path(cache_dir, phase, data_name):\n    data_path = osp.join(cache_dir, phase, name2path(data_name) + \".npy\")\n    return data_path\n\nclass DataCache(object):\n    def __init__(self, config):\n        self.config = config\n        self.cache_dir = config.get(\"cache_dir\", None)\n        if self.config.get(\"keep_in_mem\") is None:\n            self.config[\"keep_in_mem\"] = {\"default\": 1}\n        if self.config.get(\"cache_in_disk\") is None:\n            self.config[\"cache_in_disk\"] = {\"default\": 0}\n        self.datas = {\"train\": {}, \"test\": {}}\n\n    def keep_in_mem(self, phase, data_name):\n        \"\"\"\n        determine if the data for (phase, data_name) should be kept in RAM\n        if config[\"keep_in_mem\"][data_name] exist, then use it, otherwise use the default value of config[\"keep_in_mem\"] \n        \"\"\"\n        return self.config[\"keep_in_mem\"].get(data_name, self.config[\"keep_in_mem\"][\"default\"])\n\n    def cache_in_disk(self, phase, data_name):\n        \"\"\"\n        check data for (phase, data_name) is cached in disk\n        if config[\"cache_in_disk\"][data_name] exist, then use it , otherwise use default value of config[\"cache_in_disk\"]  \n        \"\"\"\n        return self.config[\"cache_in_disk\"].get(data_name, self.config[\"cache_in_disk\"][\"default\"])\n\n    def is_exist(self, phase, data_name):\n        \"\"\"\n        check data_name is generated or cashed to disk \n        \"\"\"\n        data_mem = self.datas[phase].get(data_name, None)\n        if data_mem is not None:\n            return True\n        if self.cache_dir is None:\n            return False\n        data_path = data_disk_path(self.cache_dir, phase, data_name)\n        if osp.exists(data_path):\n            return data_path\n        return None\n\n    def gets(self, phase, data_names, ignore_no_exist=False):\n        assert isinstance(data_names, list)\n        datas = []\n        for data_name in data_names:\n            datas.append(self.get(phase, data_name, ignore_no_exist=ignore_no_exist))\n        return datas\n\n    def get(self, phase, data_name, ignore_no_exist=False):\n        \"\"\"\n        get data according to data_name \n\n        Arguments\n        ---------\n        phase (str): train or test\n        data_name (str): name for tops/bottoms  \n        ignore_no_exist (bool): if True, when no data found, return None, otherwise raise e\n        \"\"\"\n        assert isinstance(data_name, basestring), \"data_name={}, type(data_name)={}\".format(data_name, type(data_name))\n        # return data if data in memory\n        data_mem = self.datas[phase].get(data_name, None)\n        if data_mem is not None:\n            return data_mem\n        # load data from disk\n        if self.cache_dir is None:\n            if ignore_no_exist:\n                return None\n            raise ValueError(\"Cache base unset, can't load data ({}->{}) from disk\".format(phase, data_name))\n        data_path = data_disk_path(self.cache_dir, phase, data_name)\n        if not osp.exists(data_path):\n            if ignore_no_exist:\n                return None\n            raise ValueError(\"Data path not exist, can't load data ({}->{}) from disk: {}\".format(phase, data_name, data_path))\n        return np.load(data_path)\n\n    def updates(self, phase, data_names, datas):\n        assert isinstance(data_names, list)\n        for i, data_name in enumerate(data_names):\n            self.update(phase, data_name, datas[i])\n\n    def update(self, phase, data_name, data):\n        \"\"\"\n        update (phase, data_name) data in cache  \n        \"\"\"\n        assert isinstance(data, np.ndarray), \"data(type={}) is not a np.ndarray!!!\".format(type(data))\n        if self.keep_in_mem(phase, data_name):\n            self.datas[phase][data_name] = data\n        if self.cache_in_disk(phase, data_name):\n            if self.cache_dir is None:\n                raise ValueError(\"Cache base unset, can't Save data ({}->{}) to disk\".format(phase, data_name))\n            data_path = data_disk_path(self.cache_dir, phase, data_name)\n            LOGGER.info(\"Updating data ({}->{}, shape={}) in disk: {}\".format(phase, data_name, data.shape, data_path))\n            check_dir(data_path);\n            np.save(data_path, data)\n\n    def reset(self, phase, X, y):\n        self.datas[phase].clear()\n        if X is not None:\n            self.update(phase, \"X\", X)\n        if y is not None:\n            self.update(phase, \"y\", y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ab3e3a569a643f04d814a60cbc2d1b8f3689b7f"},"cell_type":"code","source":"#exp utils\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\nfrom scipy.sparse import issparse\n\n#from .utils.log_utils import get_logger\n\nLOGGER = get_logger('gcforest.exp_utils')\n\ndef load_model_config(model_path, log_name=None):\n    import json\n    from .utils.config_utils import load_json\n    config = load_json(model_path)\n    if log_name is not None:\n        logger = get_logger(log_name)\n        logger.info(log_name)\n        logger.info(\"\\n\" + json.dumps(config, sort_keys=True, indent=4, separators=(',', ':')))\n    return config\n\n\ndef concat_datas(datas):\n    if type(datas) != list:\n        return datas\n    for i, data in enumerate(datas):\n        datas[i] = data.reshape((data.shape[0], -1))\n    return np.concatenate(datas, axis=1)\n\ndef data_norm(X_train, X_test):\n    X_mean = np.mean(X_train, axis=0)\n    X_std = np.std(X_train, axis=0)\n    X_train -= X_mean\n    X_train /= X_std\n    X_test -= X_mean\n    X_test /= X_std\n    return X_mean, X_std\n\ndef append_origin(X, X_origin):\n    return np.hstack(( X.reshape((X.shape[0]), -1), X_origin.reshape((X_origin.shape[0], -1)) ))\n\ndef prec_ets(n_trees, X_train, y_train, X_test, y_test, random_state=None):\n    \"\"\"\n    ExtraTrees\n    \"\"\"\n    from sklearn.ensemble import ExtraTreesClassifier\n    if not issparse(X_train):\n        X_train = X_train.reshape((X_train.shape[0], -1))\n    if not issparse(X_test):\n        X_test = X_test.reshape((X_test.shape[0], -1))\n    LOGGER.info('start predict: n_trees={},X_train.shape={},y_train.shape={},X_test.shape={},y_test.shape={}'.format(\n        n_trees, X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n    clf = ExtraTreesClassifier(n_estimators=n_trees, max_depth=None, n_jobs=-1, verbose=1, random_state=random_state)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    prec = float(np.sum(y_pred == y_test)) / len(y_test)\n    LOGGER.info('prec_ets{}={:.6f}%'.format(n_trees, prec*100.0))\n    return clf, y_pred\n\ndef prec_rf(n_trees, X_train, y_train, X_test, y_test):\n    \"\"\"\n    ExtraTrees\n    \"\"\"\n    from sklearn.ensemble import RandomForestClassifier\n    if not issparse(X_train):\n        X_train = X_train.reshape((X_train.shape[0], -1))\n    if not issparse(X_test):\n        X_test = X_test.reshape((X_test.shape[0], -1))\n    LOGGER.info('start predict: n_trees={},X_train.shape={},y_train.shape={},X_test.shape={},y_test.shape={}'.format(\n        n_trees, X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=None, n_jobs=-1, verbose=1)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    prec = float(np.sum(y_pred == y_test)) / len(y_test)\n    LOGGER.info('prec_rf{}={:.6f}%'.format(n_trees, prec*100.0))\n    return clf, y_pred\n\ndef xgb_eval_accuracy(y_pred_proba, y_true):\n    \"\"\"\n    y_true (DMatrix)\n    \"\"\"\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    y_true = y_true.get_label()\n    acc = float(np.sum(y_pred == y_true)) / len(y_pred)\n    return 'accuracy', -acc\n\ndef prec_xgb(n_trees, max_depth, X_train, y_train, X_test, y_test, learning_rate=0.1):\n    \"\"\"\n    ExtraTrees\n    \"\"\"\n    import xgboost as xgb\n    X_train = X_train.reshape((X_train.shape[0], -1))\n    X_test = X_test.reshape((X_test.shape[0], -1))\n    LOGGER.info('start predict: n_trees={},X_train.shape={},y_train.shape={},X_test.shape={},y_test.shape={}'.format(\n        n_trees, X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n    clf = xgb.XGBClassifier(n_estimators=n_trees, max_depth=max_depth, objective='multi:softprob',\n            seed=0, silent=True, nthread=-1, learning_rate=learning_rate)\n    eval_set = [(X_test, y_test)]\n    clf.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"merror\")\n    y_pred = clf.predict(X_test)\n    prec = float(np.sum(y_pred == y_test)) / len(y_test)\n    LOGGER.info('prec_xgb_{}={:.6f}%'.format(n_trees, prec*100.0))\n    return clf, y_pred\n\ndef prec_log(X_train, y_train, X_test, y_test):\n    from sklearn.linear_model import LogisticRegression\n    if not issparse(X_train):\n        X_train = X_train.reshape((X_train.shape[0], -1))\n    if not issparse(X_test):\n        X_test = X_test.reshape((X_test.shape[0], -1))\n    LOGGER.info('start predict: X_train.shape={},y_train.shape={},X_test.shape={},y_test.shape={}'.format(\n        X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n    X_train = X_train.reshape((X_train.shape[0], -1))\n    X_test = X_test.reshape((X_test.shape[0], -1))\n    clf = LogisticRegression(solver='sag', n_jobs=-1, verbose=1)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    prec = float(np.sum(y_pred == y_test)) / len(y_test)\n    LOGGER.info('prec_log={:.6f}%'.format(prec*100.0))\n    return clf, y_pred\n\ndef plot_forest_all_proba(y_proba_all, y_gt):\n    from matplotlib import pylab\n    N = len(y_gt)\n    num_tree = len(y_proba_all)\n    pylab.clf()\n    mat = np.zeros((num_tree, N))\n    LOGGER.info('mat.shape={}'.format(mat.shape))\n    for i in range(num_tree):\n        mat[i,:] = y_proba_all[i][(range(N), y_gt)]\n    pylab.matshow(mat, fignum=False, cmap='Blues', vmin=0, vmax=1.0)\n    pylab.grid(False)\n    pylab.show()\n\ndef plot_confusion_matrix(cm, label_list, title='Confusion matrix', cmap=None):\n    from matplotlib import pylab\n    cm = np.asarray(cm, dtype=np.float32)\n    for i, row in enumerate(cm):\n        cm[i] = cm[i] / np.sum(cm[i])\n    #import matplotlib.pyplot as plt\n    #plt.ion()\n    pylab.clf()\n    pylab.matshow(cm, fignum=False, cmap='Blues', vmin=0, vmax=1.0)\n    ax = pylab.axes()\n    ax.set_xticks(range(len(label_list)))\n    ax.set_xticklabels(label_list, rotation='vertical')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.set_yticks(range(len(label_list)))\n    ax.set_yticklabels(label_list)\n    pylab.title(title)\n    pylab.colorbar()\n    pylab.grid(False)\n    pylab.xlabel('Predicted class')\n    pylab.ylabel('True class')\n    pylab.grid(False)\n    pylab.savefig('test.jpg')\n    pylab.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f64f70b4e086db8062c6bffe635849b9b31586d6"},"cell_type":"code","source":"#fgnet\n# -*- coding:utf-8 -*-\n\"\"\"\nDescription: A python 2.7 implementation of gcForest proposed in [1]. A demo implementation of gcForest library as well as some demo client scripts to demostrate how to use the code. The implementation is flexible enough for modifying the model or\nfit your own datasets. \nReference: [1] Z.-H. Zhou and J. Feng. Deep Forest: Towards an Alternative to Deep Neural Networks. In IJCAI-2017.  (https://arxiv.org/abs/1702.08835v2 )\nRequirements: This package is developed with Python 2.7, please make sure all the demendencies are installed, which is specified in requirements.txt\nATTN: This package is free for academic usage. You can run it at your own risk. For other purposes, please contact Prof. Zhi-Hua Zhou(zhouzh@lamda.nju.edu.cn)\nATTN2: This package was developed by Mr.Ji Feng(fengj@lamda.nju.edu.cn). The readme file and demo roughly explains how to use the codes. For any problem concerning the codes, please feel free to contact Mr.Feng. \n\"\"\"\nimport numpy as np\nimport os, os.path as osp\nimport json\n\n# from .layers import get_layer\n# from .utils.log_utils import get_logger\n\nLOGGER = get_logger(\"gcforest.gcnet\")\n\nclass FGNet(object):\n    \"\"\"\n    GCForest : FineGrained Components\n    \"\"\"\n    def __init__(self, net_config, data_cache):\n        #net_config_str = json.dumps(net_config, sort_keys=True, indent=4, separators=(',', ':'))\n        #LOGGER.info(\"\\n\" + net_config_str)\n        self.data_cache = data_cache\n        self.inputs = net_config.get(\"inputs\", [])\n        self.check_net_config(net_config)\n        self.outputs = net_config.get(\"outputs\", [])\n\n        # layers\n        self.layers = []\n        self.name2layer = {}\n        model_disk_base = net_config.get(\"model_cache\", {}).get(\"disk_base\", None)\n        for layer_config in net_config[\"layers\"]:\n            layer = get_layer(layer_config, self.data_cache)\n            layer.model_disk_base = model_disk_base\n            self.layers.append(layer)\n            self.name2layer[layer.name] = layer\n\n\n    def fit_transform(self, X_train, y_train, X_test, y_test, train_config):\n        \"\"\"\n        delete_layer (bool): defalut=False\n            When X_test is not None and there is no need to run test, delete layer in realtime to save mem\n             \n        \"\"\"\n        LOGGER.info(\"X_train.shape={}, y_train.shape={}, X_test.shape={}, y_test.shape={}\".format(\n            X_train.shape, y_train.shape, None if X_test is None else X_test.shape, None if y_test is None else y_test.shape))\n        self.data_cache.reset(\"train\", X_train, y_train)\n        if \"test\" in train_config.phases:\n            self.data_cache.reset(\"test\", X_test, y_test)\n        for li, layer in enumerate(self.layers):\n            layer.fit_transform(train_config)\n\n    @staticmethod\n    def concat_datas(datas):\n        if type(datas) != list:\n            return datas\n        for i, data in enumerate(datas):\n            datas[i] = data.reshape((data.shape[0], -1))\n        return np.concatenate(datas, axis=1)\n\n    def transform(self, X_test):\n        LOGGER.info(\"X_test.shape={}\".format(X_test.shape))\n        self.data_cache.reset(\"test\", X_test, None)\n        for li, layer in enumerate(self.layers):\n            layer.transform()\n        return self.get_outputs(\"test\")\n\n    def score(self):\n        for li, layer in enumerate(self.layers):\n            layer.score()\n\n    def get_outputs(self, phase):\n        outputs = self.data_cache.gets(phase, self.outputs)\n        return outputs\n\n    def save_outputs(self, phase, save_y=True, save_path=None):\n        if save_path is None:\n            if self.data_cache.cache_dir is None:\n                LOGGER.error(\"save path is None and data_cache.cache_dir is None!!! don't know where to save\")\n                return\n            save_path = osp.join(self.data_cache.cache_dir, phase, \"outputs.pkl\")\n        import pickle\n        info  = \"\"\n        data_names = [name for name in self.outputs]\n        if save_y:\n            data_names.append(\"y\")\n        datas = {}\n        for di, data_name in enumerate(data_names):\n            datas[data_name] = self.data_cache.get(phase, data_name)\n            info = \"{},{}->{}\".format(info, data_name, datas[data_name].shape)\n        LOGGER.info(\"outputs.shape={}\".format(info))\n        LOGGER.info(\"Saving Outputs in {} \".format(save_path))\n        with open(save_path, \"wb\") as f:\n            pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n\n    def check_net_config(self, net_config):\n        \"\"\"\n        check net_config \n        \"\"\"\n         \n        top2layer = {}\n        name2layer = {}\n        for li, layer_config in enumerate(net_config[\"layers\"]):\n            layer_name = layer_config[\"name\"]\n            if layer_name in name2layer:\n                raise ValueError(\"layer name duplicate. layer_name={}, config1={}, config2={}\".format(\n                    layer_name, name2layer[layer_name], layer_config))\n            name2layer[layer_name] = layer_config\n\n            for bottom in layer_config[\"bottoms\"]:\n                if bottom != \"X\" and bottom != \"y\" and not bottom in self.inputs and not bottom in top2layer:\n                    raise ValueError(\"li={}, layer_config={}, bottom({}) doesn't be produced by other layers\".format(\n                        li, layer_config, bottom))\n            for top in layer_config[\"tops\"]:\n                if top in top2layer:\n                    raise ValueError(\"top duplicate. layer({}) and layer({}) have same top blob: {}\".format(\n                        top2layer[top], layer_config[\"name\"], top))\n                top2layer[top] = layer_config[\"name\"]\n         \n        outputs = net_config.get(\"outputs\", [])\n        if len(outputs) == 0:\n            LOGGER.warn(\"outputs list is empty!!!\")\n        for output in outputs:\n            if output == \"X\" or output == \"y\" or output in self.inputs or output in top2layer:\n                continue\n            raise ValueError(\"output data name not exist: output={}\".format(output))\n         \n        for layer_config in net_config[\"layers\"]:\n            if len(layer_config[\"tops\"]) > 1:\n                for top_name in layer_config[\"tops\"]:\n                    if not top_name.startswith(layer_config[\"name\"]):\n                        LOGGER.warn(\"top_name is suggested to startswith layer_name: layer_config={}\".format(layer_config))\n            else:\n                top = layer_config[\"tops\"][0]\n                if top != layer_config[\"name\"]:\n                    LOGGER.warn(\"layer_name != top_name, You should check to make sure this is what you want!!! layer_config={}\".format(layer_config))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1edb36989e912d8848c36ee1d282b7a6c8289ed4"},"cell_type":"code","source":"#config \n#from .data_cache import DataCache\n\n\nclass GCTrainConfig(object):\n    def __init__(self, train_config):\n        self.keep_model_in_mem = train_config.get(\"keep_model_in_mem\", True)\n        self.random_state = train_config.get(\"random_state\", 0)\n        self.model_cache_dir = strip(train_config.get(\"model_cache_dir\", None))\n        self.data_cache = DataCache(train_config.get(\"data_cache\", {}))\n        self.phases = train_config.get(\"phases\", [\"train\", \"test\"])\n\n        for data_name in (\"X\", \"y\"):\n            if data_name not in self.data_cache.config[\"keep_in_mem\"]:\n                self.data_cache.config[\"keep_in_mem\"][data_name] = True\n            if data_name not in self.data_cache.config[\"cache_in_disk\"]:\n                self.data_cache.config[\"cache_in_disk\"][data_name] = False\n\n\ndef strip(s):\n    if s is None:\n        return None\n    s = s.strip()\n    if len(s) == 0:\n        return None\n    return s\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"060180dac9af570a7d3e6b71112c596576eab03d"},"cell_type":"code","source":"#gcforest\nimport numpy as np\n\n# from .cascade.cascade_classifier import CascadeClassifier\n# from .config import GCTrainConfig\n# from .fgnet import FGNet\n# from .utils.log_utils import get_logger\n\nLOGGER = get_logger(\"gcforest.gcforest\")\n\n\nclass GCForest(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_config = GCTrainConfig(config.get(\"train\", {}))\n        if \"net\" in self.config:\n            self.fg = FGNet(self.config[\"net\"], self.train_config.data_cache)\n        else:\n            self.fg = None\n        if \"cascade\" in self.config:\n            self.ca = CascadeClassifier(self.config[\"cascade\"])\n        else:\n            self.ca = None\n\n    def fit_transform(self, X_train, y_train, X_test=None, y_test=None, train_config=None):\n        train_config = train_config or self.train_config\n        if X_test is None or y_test is None:\n            if \"test\" in train_config.phases:\n                train_config.phases.remove(\"test\")\n            X_test, y_test = None, None\n        if self.fg is not None:\n            self.fg.fit_transform(X_train, y_train, X_test, y_test, train_config)\n            X_train = self.fg.get_outputs(\"train\")\n            if \"test\" in train_config.phases:\n                X_test = self.fg.get_outputs(\"test\")\n        if self.ca is not None:\n            _, X_train, _, X_test, _, = self.ca.fit_transform(X_train, y_train, X_test, y_test, train_config=train_config)\n\n        if X_test is None:\n            return X_train\n        else:\n            return X_train, X_test\n\n    def transform(self, X):\n        \"\"\"\n        return:\n            if only finegrained proviede: return the result of Finegrained\n            if cascade is provided: return N x (n_trees in each layer * n_classes)\n        \"\"\"\n        if self.fg is not None:\n            X = self.fg.transform(X)\n        y_proba = self.ca.transform(X)\n        return y_proba\n\n    def predict_proba(self, X):\n        if self.fg is not None:\n            X = self.fg.transform(X)\n        y_proba = self.ca.predict_proba(X)\n        return y_proba\n\n    def predict(self, X):\n        y_proba = self.predict_proba(X)\n        y_pred = np.argmax(y_proba, axis=1)\n        return y_pred\n\n    def set_data_cache_dir(self, path):\n        self.train_config.data_cache.cache_dir = path\n\n    def set_keep_data_in_mem(self, flag):\n        \"\"\"\n        flag (bool):\n            if flag is 0, data will not be keeped in memory.\n            this is for the situation when memory is the bottleneck\n        \"\"\"\n        self.train_config.data_cache.config[\"keep_in_mem\"][\"default\"] = flag\n\n    def set_keep_model_in_mem(self, flag):\n        \"\"\"\n        flag (bool):\n            if flag is 0, model will not be keeped in memory.\n            this is for the situation when memory is the bottleneck\n        \"\"\"\n        self.train_config.keep_model_in_mem = flag\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebd3045c64252ef7105790ac750e9f5c6b734e0d","collapsed":true},"cell_type":"code","source":"import argparse\nimport numpy as np\nimport sys\nfrom keras.datasets import mnist\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef get_toy_config():\n    config = {}\n    ca_config = {}\n    ca_config[\"random_state\"] = 0\n    ca_config[\"max_layers\"] = 20\n    ca_config[\"early_stopping_rounds\"] = 3\n    ca_config[\"n_classes\"] = 2\n    ca_config[\"estimators\"] = []\n    ca_config[\"estimators\"].append(\n            {\"n_folds\": 5, \"type\": \"XGBClassifier\", \"n_estimators\": 10, \"max_depth\": 5,\n             \"objective\": \"binary:logistic\", \"silent\": True, \"nthread\": -1, \"learning_rate\": 0.1} )\n    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"ExtraTreesClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n    #ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"LogisticRegression\"})\n    config[\"cascade\"] = ca_config\n    return config\n  \n  \nconfig = get_toy_config()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7994786105b66bc53b1d19ae78e38e274679ad10"},"cell_type":"code","source":"gc = GCForest(config=config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b1fb4b551b2e3a2f24f2c8fd283f96a6b80aba4e"},"cell_type":"code","source":"import glob\nimport gc\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport datetime\nfrom functools import partial\nfrom itertools import product\nfrom multiprocessing import Pool, cpu_count\nfrom contextlib import closing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e0bb49e2bf4e6d4f078f70a940c2d3f2eb4f174d"},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\nimport lightgbm as lgb\n# import xgboost as xgb\n# from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fba9f6d6ab0c2c476284694a98e59e8e08b54f90","collapsed":true},"cell_type":"code","source":"trades_df = pd.read_csv('../input/DSG2018-qualifiers/Trade.csv')\nsubmission_df = pd.read_csv('../input/DSG2018-qualifiers/Challenge_20180423.csv')\nisin_df = pd.read_csv('../input/DSG2018-qualifiers/Isin.csv')\ncustomer_df = pd.read_csv('../input/DSG2018-qualifiers/Customer.csv')\nmarket_df = pd.read_csv('../input/DSG2018-qualifiers/Market.csv')\nprice_df = pd.read_csv(\"../input/lastdayprice/PSYPredictedV2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"556cefd45e76a27e27a02e1b034cff0cddad58f2","collapsed":true},"cell_type":"code","source":"price_df.rename(columns={\"Unnamed: 0\":\"WeekDateKey\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7d3d618b65f87fac234d4bb258f6e35484723b","collapsed":true},"cell_type":"code","source":"price_df['WeekDateKey'] = datetime.datetime.strptime('20180423','%Y%m%d')\nprice_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7337902a4f4119cb304b236e4fce7c783cb70792"},"cell_type":"code","source":"trades_df = trades_df.rename({'TradeDateKey': 'DateKey'}, axis=1)\ntrades_df.DateKey = pd.to_datetime(trades_df.DateKey, format='%Y%m%d')\ntrades_df = trades_df.sort_values(['DateKey'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"59e7bedd7a15738d6f199d4f3f3d88a453f3acad"},"cell_type":"code","source":"trades_df['WeekDateKey'] = trades_df.DateKey - pd.to_timedelta(trades_df.DateKey.dt.dayofweek, 'd')\ntrades_df = trades_df.drop_duplicates(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey', 'CustomerInterest'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbbf008b1ba14bb5f4ff77464fe3a3fbd7a580f2"},"cell_type":"code","source":"trades_df1 = trades_df[trades_df.CustomerInterest == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"113e4b5761f1d8d09c305a7126726f818b5ae18b","collapsed":true},"cell_type":"code","source":"trades_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"32b1ef8fef49cb3b4401e723d3817a7400a38021"},"cell_type":"code","source":"ngs = []\nfor i in [1, 4, 12, 16, 24]:\n    tmp = trades_df1[['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey']].copy()\n    tmp.WeekDateKey += pd.to_timedelta(7 * i, 'd')\n    ngs.append(tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2111ef0bbb704332e45e3686645eb3af952e88d1","collapsed":true},"cell_type":"code","source":"negative_sampling = pd.concat(ngs).reset_index(drop=True)\nnegative_sampling = negative_sampling[negative_sampling.WeekDateKey < '2018-04-23']\nnegative_sampling = negative_sampling.drop_duplicates(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'])\nnegative_sampling.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4afd7ad211e939750c1fd26d273ba753dd35293","collapsed":true},"cell_type":"code","source":"negative_sampling = pd.concat([\n    negative_sampling,\n    trades_df1[['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey', 'CustomerInterest']]\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cacb5d11da47da76dde0ccb79fdec28189566d02","collapsed":true},"cell_type":"code","source":"reord1 = negative_sampling[negative_sampling.duplicated(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'], keep=False)\n    &\n    (negative_sampling.CustomerInterest == 1)\n]\nreord1['CustomerInterest'] = 1\n\nreord0 = negative_sampling[~negative_sampling.duplicated(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'], keep=False)\n    &\n    (negative_sampling.CustomerInterest == 1)\n]\nreord0['CustomerInterest'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e4b3be83582090d75920501a7e243757482f181","collapsed":true},"cell_type":"code","source":"reordered = pd.concat([reord0, reord1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3279ecf857ad99a4958315fa28b5313c10c97060","collapsed":true},"cell_type":"code","source":"trades_df = reordered\ndel reordered\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"987d64c165f5504218d38d3abd846ecf1ae685ed","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"164963b146c30281569ffc918e4e8e4f4e121679","collapsed":true},"cell_type":"code","source":"trades_df.WeekDateKey = pd.to_datetime(trades_df.WeekDateKey)\n\nsubmission_df = submission_df.rename({'DateKey': 'WeekDateKey'}, axis=1)\nsubmission_df.WeekDateKey = pd.to_datetime(submission_df.WeekDateKey, format='%Y%m%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14790a2d1eef380589ef3f1842455d5f7725cc32","collapsed":true},"cell_type":"code","source":"market_df = market_df.rename({'DateKey': 'WeekDateKey'}, axis=1)\nmarket_df.WeekDateKey = pd.to_datetime(market_df.WeekDateKey, format='%Y%m%d')\nmarket_df.WeekDateKey = market_df.WeekDateKey - pd.to_timedelta(market_df.WeekDateKey.dt.dayofweek-1, 'd')\nmarket_df = pd.concat([market_df, price_df]).reset_index()\nmarket_df.Price = np.log(market_df.Price, dtype=np.float32)\nmarket_df = market_df.groupby(['WeekDateKey', 'IsinIdx']).mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d660483128e8007a0ab729e6da18ab1b0a230631","collapsed":true},"cell_type":"code","source":"isin_df = pd.read_csv(\"../input/DSG2018-qualifiers/Isin.csv\")\nisin_df.fillna(\"missing\", inplace=True)\nisin_df = isin_df.rename({'Region': 'BondRegion'}, axis=1)\nfor col in ['ActualMaturityDateKey', 'IssueDateKey']:\n    isin_df[col] = pd.to_datetime(isin_df[col], format='%Y%m%d')\n    isin_df[col] = isin_df[col].apply(lambda x: x.toordinal())\n\ncategorical_isin = list(isin_df.dtypes[isin_df.dtypes == 'object'].index) + ['TickerIdx']\nfor col in categorical_isin:\n    isin_df[col] =isin_df[col].astype('category')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f3494426efb7b4e16d0bfb1101ee23b91aeba39"},"cell_type":"code","source":"customer_df = pd.read_csv(\"../input/DSG2018-qualifiers/Customer.csv\")\ncustomer_df.fillna(\"missing\", inplace=True)\ncustomer_df = customer_df.rename({'Region': 'CustomerRegion'}, axis=1)\ncategorical_customer = list(customer_df.dtypes[customer_df.dtypes == 'object'].index)\nfor col in categorical_customer:\n    customer_df[col] = customer_df[col].astype('category')\n\ncategorical_features = categorical_customer + categorical_isin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc56bc5a36c82feecc15cdca67fa8300f5a23965","collapsed":true},"cell_type":"code","source":"trades_df = trades_df.sort_values('WeekDateKey')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e42f1d0755a05dacf9632cd411ce38c809eeb0f","collapsed":true},"cell_type":"code","source":"import pandas as pd\npd.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12e6cff1a035bca4752505d205d387a2f9390aea","collapsed":true},"cell_type":"code","source":"alldata_df = pd.concat([trades_df, submission_df], sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55e959ba37d6994292321c96f2d126a859a433ce","collapsed":true},"cell_type":"code","source":"del trades_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7943f95b104773e1df836d581f0e4ae77702dbc","collapsed":true},"cell_type":"code","source":"market_df = pd.merge(market_df, isin_df, on='IsinIdx', how='left')\nmarket_df['Order_dom'] = market_df['WeekDateKey'].apply(lambda d: (d.day-1) // 7 + 1)\nmarket_df['Order_month'] = market_df['WeekDateKey'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0954bc383c457dd5bb012df9d748b748e215cc2f"},"cell_type":"code","source":"def rolling_handler(x, feature, rolling=-1):\n    rolling = len(x) if rolling == -1 else rolling\n    return getattr(x.rolling(rolling, 1), feature)()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f57415379dbd6050f91cc676d0b6d91465a7c7b9","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"874fe924d0629ea553d4ec5c571374301ebc4e79","collapsed":true},"cell_type":"code","source":"features = ['sum', 'min', 'max', 'mean']\n\nfor groupby in categorical_isin + ['Order_dom', 'Order_month']:\n    groupby = ['IsinIdx'] + [groupby]\n    suffix = '_&_'.join(groupby)\n    market_df.sort_values(groupby + ['WeekDateKey'], inplace=True)\n    start_time = datetime.datetime.now()\n    \n    def worker_func(grouped, feature):\n        return grouped.apply(lambda x: rolling_handler(x, feature))\n    \n    grouped = market_df[groupby + ['Price']].groupby(groupby, sort=False).Price\n    partial_worker_func = partial(worker_func, grouped)\n    \n    with closing(Pool(24)) as p:\n        ret_list = p.map(partial_worker_func, features)\n        p.terminate()\n    \n    for ret, feature in zip(ret_list, features):\n        name = '{}_{}_by_{}'.format('Price', feature, suffix)\n        market_df[name] = ret\n    \n    del ret_list, grouped\n    gc.collect()\n    print(suffix, (datetime.datetime.now() - start_time).total_seconds() / 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"298b8b73e0267935c598747ae5a75ce887c5b077","collapsed":true},"cell_type":"code","source":"market_df = market_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63d2f195e6293fef96ea1876c8f9a993c93b8aa2","collapsed":true},"cell_type":"code","source":"market_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d4942cc70868453c49290a0a68d88ed1341fc823"},"cell_type":"code","source":"alldata_df = pd.merge(alldata_df, customer_df, on='CustomerIdx', how='left')\nalldata_df = pd.merge(alldata_df, market_df, on=['WeekDateKey', 'IsinIdx'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3b5641fded076f3ecd8ebdfcadfb0f9fdfb1232","collapsed":true},"cell_type":"code","source":"del isin_df, customer_df, market_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fe331aff5f98341c6500616ce9db11404f3cd01e"},"cell_type":"code","source":"alldata_df.loc[alldata_df.BuySell == 'Sell', 'Price'] *= -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d3b93a19041364d93195c2bef37305e89efbcffd"},"cell_type":"code","source":"alldata_df['BuySell'] = alldata_df['BuySell'].map({'Buy': 0, 'Sell': 1})\ncategorical_features.extend(['BuySell'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e9b33cc56fd681f2aa660ea576abdc786b17d91","collapsed":true},"cell_type":"code","source":"base_features = alldata_df.columns.difference(\n    categorical_features + ['CustomerInterest', 'IsinIdx', 'CustomerIdx', 'WeekDateKey', 'PredictionIdx']\n).tolist()\nbase_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff6edf994d075ae9c1cabd076083b3f53fd2e100"},"cell_type":"code","source":"train_df = alldata_df[(alldata_df.WeekDateKey >= '2017-01-01') & (alldata_df.WeekDateKey < '2018-04-09')]\nvalid_df = alldata_df[(alldata_df.WeekDateKey >= '2018-04-09') & (alldata_df.WeekDateKey < '2018-04-23')]\ntest_df = alldata_df[alldata_df.WeekDateKey >= '2018-04-23']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d706daf25de4e921621c299c6039bb5c70e155a5","collapsed":true},"cell_type":"code","source":"if \"TickerIdx\" in categorical_features:\n    categorical_features.remove(\"TickerIdx\")\nfor c in categorical_features:\n    print(c)\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[c].values) + list(valid_df[c].values) + list(test_df[c].values))\n    train_df[c] = lbl.transform(list(train_df[c].values))\n    valid_df[c] = lbl.transform(list(valid_df[c].values))\n    test_df[c] = lbl.transform(list(test_df[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3fe5f1273c60cb2b687e6040d1ea91c2f7dd71c","collapsed":true},"cell_type":"code","source":"X_train = train_df[base_features + categorical_features]\nX_valid = valid_df[base_features + categorical_features]\nX_test = test_df[base_features + categorical_features]\ny_train = train_df['CustomerInterest']\ny_valid = valid_df['CustomerInterest']\ny_test = test_df['CustomerInterest']\nPredictionIdx = test_df['PredictionIdx']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b35a700aeea2bd6ec570738037db7699df2758e0"},"cell_type":"code","source":"params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': .01,\n    'num_leaves': 32,\n    'max_depth': 12,\n    'feature_fraction': 0.35,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 2,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d2cfb2a525be760ea72b984cada5a42280571c42"},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid = lgb.Dataset(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"747dab95ba42cbb57a19e09908da48973069467c","scrolled":false,"collapsed":true},"cell_type":"code","source":"gbm = lgb.train(params, lgb_train, categorical_feature=categorical_features, valid_sets=[lgb_train, lgb_valid], valid_names=['train','valid'], num_boost_round=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a923d5dd38c43e03244dd12ea7397f245f40c7d5"},"cell_type":"code","source":"preds = gbm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e8c311ef1be010bb2b83961c40d892fdcfc40f27"},"cell_type":"code","source":"pd.DataFrame({'PredictionIdx': PredictionIdx, 'CustomerInterest': preds}).to_csv('lgb_price_ma_2.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c25245c2824e226011cff370782865cf5c9fc7a","collapsed":true},"cell_type":"code","source":"import os\nos.listdir(\"../working/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd6c4dc3ab4233794086b81a088eb4f2a095728f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}