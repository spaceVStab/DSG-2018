{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import glob\nimport gc\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport datetime\nfrom functools import partial\nfrom itertools import product\nfrom multiprocessing import Pool, cpu_count\nfrom contextlib import closing\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b80e4a9ab4eab2b24fa9f62e6e2412deecf9d546"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, merge, Reshape, Dropout, Input, Flatten, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import EarlyStopping\nfrom sklearn import metrics, ensemble, neighbors, linear_model, tree, model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"213017d221d263c43549eb5438303f6fc1d27fcd"},"cell_type":"code","source":"trades_df = pd.read_csv('../input/Trade.csv')\nsubmission_df = pd.read_csv('../input/Challenge_20180423.csv')\nisin_df = pd.read_csv('../input/Isin.csv')\ncustomer_df = pd.read_csv('../input/Customer.csv')\nmarket_df = pd.read_csv('../input/Market.csv')\n\ntrades_df = trades_df.rename({'TradeDateKey': 'DateKey'}, axis=1)\ntrades_df.DateKey = pd.to_datetime(trades_df.DateKey, format='%Y%m%d')\ntrades_df = trades_df.sort_values(['DateKey'])\n\ntrades_df['WeekDateKey'] = trades_df.DateKey - pd.to_timedelta(trades_df.DateKey.dt.dayofweek, 'd')\ntrades_df = trades_df.drop_duplicates(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey', 'CustomerInterest'])\n\ntrades_df1 = trades_df[trades_df.CustomerInterest == 1]\n\n#trades_df.shape\n\nngs = []\nfor i in [1, 4, 12, 16, 24]:\n    tmp = trades_df1[['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey']].copy()\n    tmp.WeekDateKey += pd.to_timedelta(7 * i, 'd')\n    ngs.append(tmp)\n\nnegative_sampling = pd.concat(ngs).reset_index(drop=True)\nnegative_sampling = negative_sampling[negative_sampling.WeekDateKey < '2018-04-23']\nnegative_sampling = negative_sampling.drop_duplicates(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'])\n#negative_sampling.shape\n\nnegative_sampling = pd.concat([\n    negative_sampling,\n    trades_df1[['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey', 'CustomerInterest']]\n])\n\nreord1 = negative_sampling[negative_sampling.duplicated(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'], keep=False)\n    &\n    (negative_sampling.CustomerInterest == 1)\n]\nreord1['CustomerInterest'] = 1\n\nreord0 = negative_sampling[~negative_sampling.duplicated(['CustomerIdx', 'IsinIdx', 'BuySell', 'WeekDateKey'], keep=False)\n    &\n    (negative_sampling.CustomerInterest == 1)\n]\nreord0['CustomerInterest'] = 0\n\nreordered = pd.concat([reord0, reord1])\n\ntrades_df = reordered\ndel reordered\n#gc.collect()\n\ntrades_df.WeekDateKey = pd.to_datetime(trades_df.WeekDateKey)\n\nsubmission_df = submission_df.rename({'DateKey': 'WeekDateKey'}, axis=1)\nsubmission_df.WeekDateKey = pd.to_datetime(submission_df.WeekDateKey, format='%Y%m%d')\n\nmarket_df = market_df.rename({'DateKey': 'WeekDateKey'}, axis=1)\nmarket_df.WeekDateKey = pd.to_datetime(market_df.WeekDateKey, format='%Y%m%d')\nmarket_df.WeekDateKey = market_df.WeekDateKey + pd.to_timedelta(7 - market_df.WeekDateKey.dt.dayofweek, 'd')\nmarket_df.Price = np.log(market_df.Price, dtype=np.float32)\nmarket_df = market_df.groupby(['WeekDateKey', 'IsinIdx']).mean().reset_index()\n\nisin_df = pd.read_csv(\"../input/Isin.csv\")\nisin_df.fillna(\"missing\", inplace=True)\nisin_df = isin_df.rename({'Region': 'BondRegion'}, axis=1)\nfor col in ['ActualMaturityDateKey', 'IssueDateKey']:\n    isin_df[col] = pd.to_datetime(isin_df[col], format='%Y%m%d')\n    isin_df[col] = isin_df[col].apply(lambda x: x.toordinal())\n\ncategorical_isin = list(isin_df.dtypes[isin_df.dtypes == 'object'].index) + ['TickerIdx']\n# for col in categorical_isin:\n#     isin_df[col] =isin_df[col].astype('category')\n\ncustomer_df = pd.read_csv(\"../input/Customer.csv\")\ncustomer_df.fillna(\"missing\", inplace=True)\ncustomer_df = customer_df.rename({'Region': 'CustomerRegion'}, axis=1)\ncategorical_customer = list(customer_df.dtypes[customer_df.dtypes == 'object'].index)\n# for col in categorical_customer:\n#     customer_df[col] = customer_df[col].astype('category')\n\ncategorical_features = categorical_customer + categorical_isin\n\ntrades_df = trades_df.sort_values('WeekDateKey')\n\nalldata_df = pd.concat([trades_df, submission_df], sort=False).reset_index(drop=True)\n\ndel trades_df\ngc.collect()\n\nmarket_df = pd.merge(market_df, isin_df, on='IsinIdx', how='left')\nmarket_df['Order_dom'] = market_df['WeekDateKey'].apply(lambda d: (d.day-1) // 7 + 1)\nmarket_df['Order_month'] = market_df['WeekDateKey'].dt.month\n\ndef rolling_handler(x, feature, rolling=-1):\n    rolling = len(x) if rolling == -1 else rolling\n    return getattr(x.rolling(rolling, 1), feature)()\n\nfeatures = ['sum', 'min', 'max', 'mean']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60970d38e6624a611e84844ba6ee3f6c89e6b128"},"cell_type":"code","source":"for groupby in categorical_isin + ['Order_dom', 'Order_month']:\n    groupby = ['IsinIdx'] + [groupby]\n    suffix = '_&_'.join(groupby)\n    market_df.sort_values(groupby + ['WeekDateKey'], inplace=True)\n    start_time = datetime.datetime.now()\n    \n    def worker_func(grouped, feature):\n        return grouped.apply(lambda x: rolling_handler(x, feature))\n    \n    grouped = market_df[groupby + ['Price']].groupby(groupby, sort=False).Price\n    partial_worker_func = partial(worker_func, grouped)\n    \n    with closing(Pool(24)) as p:\n        ret_list = p.map(partial_worker_func, features)\n        p.terminate()\n    \n    for ret, feature in zip(ret_list, features):\n        name = '{}_{}_by_{}'.format('Price', feature, suffix)\n        market_df[name] = ret\n    \n    del ret_list, grouped\n    gc.collect()\n    print(suffix, (datetime.datetime.now() - start_time).total_seconds() / 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1be4721e8d674a6ef8903d5f186ede7505489e71","collapsed":true},"cell_type":"code","source":"trades_df = pd.concat([reord0, reord1])\nalldata_df = pd.concat([trades_df, submission_df], sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c4b9237ad2a37433554db21e35405c1a7a466f","collapsed":true},"cell_type":"code","source":"market_df = market_df.dropna()\n\nalldata_df = pd.merge(alldata_df, customer_df, on='CustomerIdx', how='left')\nalldata_df = pd.merge(alldata_df, market_df, on=['WeekDateKey', 'IsinIdx'], how='left')\n\n# del isin_df, customer_df, market_df\n# gc.collect()\n\nalldata_df.loc[alldata_df.BuySell == 'Sell', 'Price'] *= -1\n\nalldata_df['BuySell'] = alldata_df['BuySell'].map({'Buy': 0, 'Sell': 1})\ncategorical_features.extend(['BuySell'])\n\nbase_features = alldata_df.columns.difference(\n    categorical_features + ['CustomerInterest', 'IsinIdx', 'CustomerIdx', 'WeekDateKey', 'PredictionIdx']\n).tolist()\n#base_features\n\nalldata_df.drop(['TickerIdx'], axis=1, inplace=True)\n\nfor c in base_features:\n    alldata_df[c].fillna(alldata_df[c].mean(), inplace=True)\n\ntrain_df = alldata_df[(alldata_df.WeekDateKey >= '2016-01-01') & (alldata_df.WeekDateKey < '2018-04-09')]\nvalid_df = alldata_df[(alldata_df.WeekDateKey >= '2018-04-09') & (alldata_df.WeekDateKey < '2018-04-23')]\ntest_df = alldata_df[alldata_df.WeekDateKey >= '2018-04-23']\n\n#train_df['Price'].isnull().value_counts()\n\n#categorical_features.append(\"BuySell\")\n#categorical_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e41f1762d8c9f546ae944a3d14f34621f87d30e"},"cell_type":"code","source":"if \"TickerIdx\" in categorical_features:\n    categorical_features.remove(\"TickerIdx\")\nfor c in categorical_features:\n    print(c)\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[c].values) + list(valid_df[c].values) + list(test_df[c].values))\n    train_df[c] = lbl.transform(list(train_df[c].values))\n    valid_df[c] = lbl.transform(list(valid_df[c].values))\n    test_df[c] = lbl.transform(list(test_df[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbac634d1e11071346d13d70f067d38e576bb7d"},"cell_type":"code","source":"#categorical_features.remove(\"TickerIdx\")\nX_train = train_df[base_features + categorical_features]\nX_valid = valid_df[base_features + categorical_features]\nX_test = test_df[base_features + categorical_features]\ny_train = train_df['CustomerInterest'].values\ny_valid = valid_df['CustomerInterest'].values\ny_test = test_df['CustomerInterest'].fillna(0).values\nPredictionIdx = test_df['PredictionIdx']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c73c7e084dcd486e79efcca79803c0b1b300508"},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df3015dc03c26ea4876e05bf604339f1a573d9c4","collapsed":true},"cell_type":"code","source":"embed_cols = categorical_features\nlen_embed_cols = []\nfor col in embed_cols:\n    len_embed_cols.append(X_train[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"34c275cc27ce04060c632c9bece986b4522c5fe2"},"cell_type":"code","source":"def build_embedding_network(len_embed_cols):\n    \n    model_out = []\n    model_in  = []\n    \n    for dim in len_embed_cols:\n        input_dim = Input(shape=(1,))\n        embed_dim = Embedding(dim, dim//2, input_length=1)(input_dim)\n        embed_dim = Dropout(0.25)(embed_dim)\n        embed_dim = Reshape((dim//2,))(embed_dim)\n        model_out.append(embed_dim)\n        model_in.append(input_dim)\n    \n    input_num = Input(shape=(68,), dtype='float32')\n    outputs = Concatenate(axis=1)([*model_out, input_num])\n    \n#     outputs = (Dense(128))(outputs) \n#     outputs = (Activation('relu'))(outputs)\n#     outputs = (Dropout(.35))(outputs)\n    outputs = (Dense(64))(outputs)\n    outputs = (Activation('relu'))(outputs)\n    outputs = (Dropout(.15))(outputs)\n    outputs = (Dense(32))(outputs) \n    outputs = (Activation('relu'))(outputs)\n    outputs = (Dropout(.15))(outputs)\n    outputs = (Dense(1))(outputs)\n    outputs = (Activation('sigmoid'))(outputs)\n    \n    model = Model([*model_in, input_num], outputs)\n\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"94b5ac864b27825c4ffb41de2a127306ac322c2a"},"cell_type":"code","source":"def preproc(X_train, X_val, X_test):\n\n    input_list_train = []\n    input_list_val = []\n    input_list_test = []\n    \n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in embed_cols:\n        raw_vals = np.unique(X_train[c])\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list_train.append(X_train[c].map(val_map).values)\n        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n        \n    #the rest of the columns\n    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n    input_list_train.append(X_train[other_cols].values)\n    input_list_val.append(X_val[other_cols].values)\n    input_list_test.append(X_test[other_cols].values)\n    \n    return input_list_train, input_list_val, input_list_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18ef170beb0d57b3b429cc3cb2048a6d5d63b6f6"},"cell_type":"code","source":"num_cols = base_features\n\n# Fit the scaler only on train data\n\nscaler = MinMaxScaler().fit(alldata_df[num_cols])\nX_train.loc[:,num_cols] = scaler.transform(X_train[num_cols])\nX_valid.loc[:,num_cols] = scaler.transform(X_valid[num_cols])\nX_test.loc[:,num_cols] = scaler.transform(X_test[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54ddc0dea7394e29bf22373a15f1d7021b7ccf50"},"cell_type":"code","source":"for c in categorical_features:\n    print(c)\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[c].values) + list(valid_df[c].values) + list(test_df[c].values))\n    train_df[c] = lbl.transform(list(train_df[c].values))\n    valid_df[c] = lbl.transform(list(valid_df[c].values))\n    test_df[c] = lbl.transform(list(test_df[c].values))\n    train_df[c] = train_df[c].apply(pd.to_numeric)\n    valid_df[c] = valid_df[c].apply(pd.to_numeric)\n    test_df[c] = test_df[c].apply(pd.to_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ea251282d5df813ada96ec7ad52135597b89e32","scrolled":false},"cell_type":"code","source":"proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train, X_valid, X_test)\nNN = build_embedding_network(len_embed_cols)\ncallbacks = [EarlyStopping(monitor='val_loss', patience=5)]\nNN.fit(proc_X_train_f, y_train, epochs=50, batch_size=1024, verbose=1,callbacks=callbacks,validation_data=(proc_X_val_f, y_valid))\n# val_preds = NN.predict(proc_X_val_f)\n# v_auc  = roc_auc_score(val_y.values, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304f3d5abccf02ae50c1609e6ad45dfef2ba3483","collapsed":true},"cell_type":"code","source":"val_preds = NN.predict(proc_X_test_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40d94f9b8d4ad9f2eccf45c17e1c8e454dd2144d"},"cell_type":"code","source":"pd.DataFrame({'PredictionIdx': PredictionIdx, 'CustomerInterest': val_preds[:,0]}).to_csv('nn_ma_2.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e093976a9aab5dcf4110b4be710632b7c2e82c90"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}